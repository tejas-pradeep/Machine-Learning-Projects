{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zv2wdgDGOJcr"
   },
   "source": [
    "## Fall 2021 CS4641/CS7641 A Homework 3\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: Tuesday, November 9, 11:59 pm AOE\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "* No unapproved extension of the deadline is allowed. Late submissions will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "<font color='darkred'>\n",
    "* Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, paraphrase, or submit materials created or published by others as if you created the materials. All materials submitted must be your own.</font>\n",
    "<font color='darkred'>\n",
    "* All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the institute’s Academic Integrity procedures (e.g., reported to and directly handled by the Office of Student Integrity (OSI)). **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A56xiji5OJct"
   },
   "source": [
    "## Instructions for the assignment \n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You can directly type Latex equations into markdown cells.\n",
    "    \n",
    "- If a question requires a picture you can use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "\n",
    "- Your write up must be submitted in PDF form. You may use either Latex,  markdown, or any word processing software. <font color = 'darkred'>We will **NOT** accept handwritten work. </font> Make sure that your work is formatted correctly. For example, submit $\\sum_{i=0} x_i$ instead of \\text{sum\\_\\{i=0\\} x\\_i}\n",
    "- When submitting the non-programming part of your assignment, you must correctly map pages of your PDF to each question/subquestion to reflect where they appear. Improperly mapped questions may not be graded correctly.\n",
    "- Discussion is encouraged on Edstem as part of the Q/A. You may discuss high-level ideas with other students at the \"whiteboard\" level (e.g. how cross validation works, using matmul instead of dot) and review any relevant materials online. However, all assignments should be done individually and each student must write up and submit their own answers.\n",
    "- **Graduate Students**: You are required to complete any sections marked as Bonus for Undergrads  \n",
    "\n",
    "## Using the autograder\n",
    "\n",
    "- You will find three assignments (for grads) on Gradescope that correspond to HW3: \"Assignment 3 Programming\", \"Assignment 3 - Non-programming\" and \"Assignment 3 Programming - Bonus for all\". Undergrads will have an additional assignment called \"Assignment 3 Programming - Bonus for Undergrads\"\n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- You will submit your code for the autograder in the Assignment 3 Programming sections. Please refer to the Deliverables and Point Distribution section for what parts are considered required, bonus for undergrads, and bonus for all.\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "<!-- No changes needed on the above section -->\n",
    "\n",
    "- **For the \"Assignment 3 - Non-programming\" part, you will download your Jupyter Notebook as html and submit it as a PDF on Gradescope. To download the notebook as html, click on \"File\" on the top left corner of this page and select \"Download as > html\". Then, open the html file and print to PDF.**  Please refer to the Deliverables and Point Distribution section for an outline of the non-programming questions.\n",
    "- **When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0d4OUW-OJcu"
   },
   "source": [
    "## Deliverables and Points Distribution\n",
    "### Q1: Image Compression [30pts]\n",
    "#### Deliverables: <font color = 'green'>imgcompression.py and printed results</font>\n",
    "- **1.1 Image Compression** [20 pts] - _programming_\n",
    "\n",
    "    - svd [5pts]\n",
    "\n",
    "    - rebuild_svd [5pts]\n",
    "\n",
    "    - compression_ratio [5pts]\n",
    "\n",
    "    - recovered_variance_proportion [5pts]\n",
    "\n",
    "- **1.2 Black and White** [5 pts] _non-programming_\n",
    "\n",
    "- **1.3 Color Image** [5 pts] _non-programming_\n",
    "\n",
    "### Q2: Understanding PCA [20pts]\n",
    "#### Deliverables: <font color = 'green'>pca.py and written portion</font>\n",
    "\n",
    "- **2.1 PCA Implementation** [10 pts] - _programming_\n",
    "\n",
    "    - fit [5pts]\n",
    "\n",
    "    - transform [2pts]\n",
    "\n",
    "    - transform_rv [3pts]\n",
    "\n",
    "- **2.2 Visualize** [5 pts] _non-programming_\n",
    "\n",
    "- **2.3 Weaknesses of PCA** [5 pts] _non-programming_\n",
    "\n",
    "### Q3: Regression and Regularization [60 + (20 bonus for undergrads) pts] \n",
    "#### Deliverables: <font color = 'green'>regression.py and Written portion</font>\n",
    "\n",
    "- **3.1 Regression and Regularization Implementations** [30 pts + 20 pts Bonus for Undergrad] - _programming_\n",
    "\n",
    "    - RMSE [5pts]\n",
    "\n",
    "    - Construct Poly Features 1D [2pts]\n",
    "\n",
    "    - Construct Poly Features 2D [3pts]\n",
    "\n",
    "    - Prediction [5pts]\n",
    "\n",
    "    - Linear Fit Closed Form [5pts]\n",
    "\n",
    "    - Ridge Fit Closed Form [5pts]\n",
    "\n",
    "    - Cross Validation [5pts]\n",
    "\n",
    "    - Linear Stochastic Descent [5pts] *Bonus for Undergrad*\n",
    "\n",
    "    - Linear Stochastic Gradient Descent [5pts] *Bonus for Undergrad*\n",
    "\n",
    "    - Ridge Gradient Descent [5pts] *Bonus for Undergrad*\n",
    "\n",
    "    - Ridge Stochastic Gradient Descent [5pts] *Bonus for Undergrad*\n",
    "\n",
    "- **3.2 About RMSE** [3 pts] _non-programming_\n",
    "\n",
    "- **3.3 Testing: General Functions and Linear Regression** [5 pts] _non-programming_\n",
    "\n",
    "- **3.4 Testing: Ridge Regression** [5 pts] _non-programming_\n",
    "\n",
    "- **3.5 Cross Validation** [7 pts] _non-programming_\n",
    "\n",
    "- **3.6 Noisy Input Samples in Linear Regression** [10 pts] _non-programming_\n",
    "\n",
    "### Q4: Naive Bayes Classification [25pts] \n",
    "#### Deliverables: <font color = 'green'>nb.py and Written portion</font>\n",
    "\n",
    "- **4.1 Naive Bayes in Marketing** [5 pts] _non-programming_\n",
    "\n",
    "- **4.2 Amazon Product Ratings from Product Reviews** [15 pts] - _programming_\n",
    "\n",
    "    - priors_prob [6pts]\n",
    "\n",
    "    - likelihood_ratio [6pts]\n",
    "\n",
    "    - analyze_star_rating [3pts]\n",
    "\n",
    "- **4.3 Accuracy result analysis** [5 pts] _non-programming_\n",
    "\n",
    "### Q5: Noise in PCA and Linear Regression [15pts] \n",
    "#### Deliverables: <font color = 'green'>Written portion</font>\n",
    "\n",
    "- **5.1 Slope Functions** [5 pts] _non-programming_\n",
    "\n",
    "- **5.2 Error in Y and Error in X and Y** [5 pts] _non-programming_\n",
    "\n",
    "- **5.3 Analysis** [5 pts] _non-programming_\n",
    "\n",
    "### Q6: Feature Reduction.py [25pts Bonus for All] \n",
    "#### Deliverables: <font color = 'green'>feature_reduction.py and Written portion</font>\n",
    "\n",
    "- **6.1 Feature Reduction** [18 pts] - _programming_\n",
    "\n",
    "    - forward_selection [9pts]\n",
    "\n",
    "    - backward_elimination [9pts]\n",
    "\n",
    "- **6.2 Feature Selection - Discussion** [7 pts] _non-programming_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQ7VM2hKOJcv"
   },
   "source": [
    "## 0 Set up\n",
    "This notebook is tested under [python 3. * . *](https://www.python.org/downloads/release/python-368/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [matplotlib](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "- [sklearn](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "- [Axes3D](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "\n",
    "There is also a [VS Code and Anaconda Setup Tutorial](https://edstem.org/us/courses/8220/discussion/567423) on Ed under the \"Links\" category\n",
    "\n",
    "Please implement the functions that have \"raise NotImplementedError\", and after you finish the coding, please delete or comment \"raise NotImplementedError\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# This is cell which sets up some of the modules you might need \n",
    "# Please do not change the cell or import any additional packages. \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_digits, load_breast_cancer, load_iris, load_wine\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "import gzip\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Q1: Image Compression [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images data and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# load Image\n",
    "image = plt.imread(\"./data/hw3_image_compression.jpg\")/255\n",
    "#plot image\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "def rgb2gray(rgb):   \n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "# plot several images\n",
    "plt.imshow(rgb2gray(image), cmap=plt.cm.bone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Image compression [20pts]  <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "SVD is a dimensionality reduction technique that allows us to compress images by throwing away the least important information.  \n",
    "\n",
    "Higher singular values capture greater variance and thus capture greater information from the corresponding singular vector. To perform image compression, apply SVD on each matrix and get rid of the small singular values to compress the image. The loss of information through this process is negligible and the difference between the images can hardly be spotted. \n",
    "\n",
    "For example, the variance captured by the first component $$\\frac{\\sigma_1^2}{\\sum_{i=1}^n \\sigma_i^2}$$ where $\\sigma_i$ is the $i^{th}$ singular value.\n",
    "\n",
    "In the <strong>imgcompression.py</strong> file, complete the following functions:\n",
    "  * <strong>svd</strong>: You may use <samp>np.linalg.svd</samp> in this function and set <samp>full_matrices=True</samp> which is the default value\n",
    "  * <strong>rebuild_svd</strong>\n",
    "  * <strong>compression_ratio</strong>\n",
    "  * <strong>recovered_variance_proportion</strong>\n",
    "\n",
    "**Hint 1:** http://timbaumann.info/svd-image-compression-demo/ is a useful article on image compression and compression ratio.  \n",
    "**Hint 2:** If you have never used <samp>np.linalg.svd</samp> it might be helpful to read [Numpy's SVD documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html) and note the particularities of the V matrix and that it is returned already transposed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Black and white [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Use your implementation to generate a set of images compressed to different degrees.  \n",
    "<strong>Include the images in your non-programming submission of the assignment.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from imgcompression import ImgCompression\n",
    "\n",
    "imcompression = ImgCompression()\n",
    "bw_image = rgb2gray(image)\n",
    "U, S, V = imcompression.svd(bw_image)\n",
    "component_num = [1,2,5,10,20,40,80,160,256]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "# plot several images\n",
    "i=0\n",
    "for k in component_num:\n",
    "    img_rebuild = imcompression.rebuild_svd(U, S, V, k)\n",
    "    c = np.around(imcompression.compression_ratio(bw_image, k), 4)\n",
    "    r = np.around(imcompression.recovered_variance_proportion(S, k), 3)\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img_rebuild, cmap=plt.cm.bone)\n",
    "    ax.set_title(f\"{k} Components\")\n",
    "    ax.set_xlabel(f\"Compression: {c},\\nRecovered Variance: {r}\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Color image [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Use your implementation to generate a set of images compressed to different degrees.  \n",
    "**Include the images in your non-programming submission of the assignment.**\n",
    "\n",
    "<b>Note:</b> You might get warning \"Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\" This warning is acceptable since while rebuilding some of the pixels may go above 1.0. You should see similar image to original even with such clipping.\n",
    "\n",
    "**Hint 1:** Make sure your implementation of <samp>recovered_variance_proportion</samp> returns an array of 3 floats for a color image.  \n",
    "**Hint 2:** Try performing SVD on the individual color channels and then stack the individual channel U, S, V matrices.  \n",
    "**Hint 3:** You may need separate implementations for a color or grayscale image in the same function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "from imgcompression import ImgCompression\n",
    "\n",
    "imcompression = ImgCompression()\n",
    "U, S, V = imcompression.svd(image)\n",
    "\n",
    "# component_num = [1,2,5,10,20,40,80,160,256]\n",
    "component_num = [1,2,5,10,20,40,80,160,256]\n",
    "\n",
    "fig = plt.figure(figsize=(18, 18))\n",
    "\n",
    "# plot several images\n",
    "i=0\n",
    "for k in component_num:\n",
    "    img_rebuild = imcompression.rebuild_svd(U, S, V, k)\n",
    "    c = np.around(imcompression.compression_ratio(image, k), 4)\n",
    "    r = np.around(imcompression.recovered_variance_proportion(S, k), 3)\n",
    "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(img_rebuild)\n",
    "    ax.set_title(f\"{k} Components\")\n",
    "    ax.set_xlabel(f\"Compression: {np.around(c,4)},\\nRecovered Variance:  R: {r[0]}  G: {r[1]}  B: {r[2]}\")\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Q2: Understanding PCA [20 pts]\n",
    "\n",
    "### 2.1 Implementation [10 pts] <span style=\"color:blue\">**[P]**</span>\n",
    "\n",
    "[Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) (PCA) is another dimensionality reduction technique that reduces dimensions by eliminating small variance eigenvalues and their vectors. With PCA, we center the data first by subtracting the mean. Each singular value tells us how much of the variance of a matrix (e.g. image) is captured in each component. In this problem, we will investigate how PCA can be used to improve features for regression and classification tasks and how the data itself affects the behavior of PCA.  \n",
    "\n",
    "Implement PCA. In the <strong>pca.py</strong> file, complete the following functions:\n",
    "  * <strong>fit</strong>: You may use <samp>np.linalg.svd</samp>. Set <samp>full_matrices=False</samp>\n",
    "  * <strong>transform</strong>\n",
    "  * <strong>transform_rv</strong>: You may find <samp>np.cumsum</samp> helpful for this function.\n",
    "\n",
    "Assume a dataset is composed of N datapoints, each of which has D features with D < N. The dimension of our data would be D. It is possible, however, that many of these dimensions contain redundant information. Each feature explains part of the variance in our dataset. Some features may explain more variance than others.\n",
    "\n",
    "#### In the <strong>pca.py</strong> file, complete the PCA class by completing functions fit, transform and transform_rv.\n",
    "\n",
    "### 2.2 Visualize [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "PCA is used to transform multivariate data tables into smaller sets so as to observe the hidden trends and variations in the data. Here you will visualize two datasets (iris and wine) using PCA. Use the above implementation of PCA and reduce the datasets such that they contain only two features. Make 2-D scatter plots of the data points using these features. Make sure to differentiate the data points according to their true labels. The datasets have already been loaded for you. In addition, return the retained variance obtained from the reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Use PCA for visualization of breast cancer data\n",
    "from pca import PCA\n",
    "bc_data = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "X = bc_data[0]\n",
    "y = bc_data[1]\n",
    "\n",
    "plt.title('Breast Cancer Dataset with Dimensionality Reduction')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "PCA().visualize(X,y)\n",
    "print('*In this plot, the 0 points are malignant and the 1 points are benign.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Use PCA for visualization of masked and unmasked images\n",
    "\n",
    "X = np.load('./data/smallflat.npy')\n",
    "y = np.load('./data/masked_labels.npy')\n",
    "\n",
    "plt.title('Facemask Dataset Visualization with Dimensionality Reduction')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "PCA().visualize(X,y)\n",
    "print('*In this plot, the 0 points are unmasked images and the 1 points are masked images.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the distinct separation between the data points with different labels in both plots above.\n",
    "\n",
    "Now you will use PCA on an actual real-world dataset. We will use your implementation of PCA function to reduce the dataset with 99% retained variance and use it to obtain the reduced features. On the reduced dataset, we will use logistic and linear regression to compare results between PCA and non-PCA datasets. Run the following cells to see how PCA works on regression and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "#load the dataset \n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "print(\"data shape before PCA \",X.shape)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform_rv(X)\n",
    "\n",
    "print(\"data shape with PCA \",X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Use logistic regression to predict classes for test set\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print('Accuracy before PCA: {:.5f}'.format(accuracy_score(y_test, \n",
    "                                                preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# Use logistic regression to predict classes for test set\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "print('Accuracy after PCA: {:.5f}'.format(accuracy_score(y_test, \n",
    "                                                preds.argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "def apply_regression(X_train, y_train, X_test):\n",
    "    ridge = Ridge()\n",
    "    weight = ridge.fit(X_train, y_train)\n",
    "    y_pred = ridge.predict(X_test)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "#load the dataset \n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform_rv(X, retained_variance = 0.9)\n",
    "print(\"data shape with PCA \",X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# Train, test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "#Ridge regression without PCA\n",
    "y_pred = apply_regression(X_train, y_train, X_test)\n",
    "\n",
    "# calculate RMSE \n",
    "rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print('RMSE score using Ridge Regression before PCA: {:.5}'.format(rmse_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "#Ridge regression with PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, random_state=42)\n",
    "\n",
    "#use Ridge Regression for getting predicted labels\n",
    "y_pred = apply_regression(X_train,y_train,X_test)\n",
    "\n",
    "#calculate RMSE \n",
    "rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print('RMSE score using Ridge Regression after PCA: {:.5}'.format(rmse_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Weakenesses of PCA [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "Sometimes, PCA does not improve performance. Let's run PCA on a different dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "X = np.load('./data/heart_disease_features.npy')\n",
    "y = np.load('./data/heart_disease_labels.npy')\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "X_pca = pca.transform_rv(X, retained_variance = 0.9)\n",
    "print(\"data shape with PCA \",X_pca.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=42)\n",
    "\n",
    "#Ridge regression without PCA\n",
    "y_pred = apply_regression(X_train, y_train, X_test)\n",
    "\n",
    "# calculate RMSE \n",
    "rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print('RMSE score using Ridge Regression before PCA: {:.5}'.format(rmse_score))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=.3, random_state=42)\n",
    "\n",
    "#use Ridge Regression for getting predicted labels\n",
    "y_pred = apply_regression(X_train,y_train,X_test)\n",
    "\n",
    "#calculate RMSE \n",
    "rmse_score = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "print('RMSE score using Ridge Regression after PCA: {:.5}'.format(rmse_score))\n",
    "\n",
    "PCA().visualize(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide one reason as to why PCA does not improve performance for this example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8RXOTa_lpYN"
   },
   "source": [
    "## 3 Polynomial regression and regularization [60 pts + 20 pts bonus for CS 4641] <span style=\"color:blue\">**[P]**</span> | <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Cq4yJhTlpYN",
    "tags": []
   },
   "source": [
    "### 3.1 Regression and regularization implementations [30 pts + 20 pts bonus for CS 4641] <span style=\"color:blue\">**[P]**</span>\n",
    "We have three methods to fit linear and ridge regression models: 1) close form; 2) gradient descent (GD); 3) Stochastic gradient descent (SGD). For undergraduate students, you are required to implement the closed form for linear regression and for ridge regression, the others 4 methods are bonus parts. For graduate students, you are required to implement all of them. We use the term weight in the following code. Weights and parameters ($\\theta$) have the same meaning here. We used parameters ($\\theta$) in the lecture slides.\n",
    "\n",
    "In the <strong>regression.py</strong> file, complete the Regression class by completing functions rmse, construct_polynomial_feates, predict first. Then, construct linear_fit_closed, linear_fit_GD, linear_fit_SGD for linear regression and ridge_fit_closed, ridge_fit_GD, and ridge_fit_SGD for ridge regression.  For undergraduate students, you are required to implement the closed form for linear regression and for ridge regression, the other 4 methods are bonus questions. <strong>For graduate students, you are required to implement all of them.</strong> The points for each function is in regression.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_DoBd-KtN7w"
   },
   "outputs": [],
   "source": [
    "from regression import Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcaRwCsmlpYO"
   },
   "source": [
    "### 3.2 About RMSE [3 pts] <span style=\"color:green\">**[W]**</span>\n",
    "What is a good RMSE value? If we normalize our labels between 0 and 1, what does it mean when normalized RMSE = 1? Please provide an example with your explanation.\n",
    "\n",
    "**Hint**: Think of the way that you can enforce your RMSE = 1. Note that you can not change the actual labels to make RMSE = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5AioXetlpYO"
   },
   "source": [
    "### 3.3 Testing: general functions and linear regression [5 pts] <span style=\"color:green\">**[W]**</span>\n",
    "In this section. we will test the performance of the linear regression. As long as your test rmse score is close to the TA's answer (TA's answer $\\pm 0.5$), you can get full points.\n",
    "Let's first construct a dataset for polynomial regression.\n",
    "\n",
    "In this case, we construct the polynomial features up to degree 5.\n",
    "Each data sample consists of two features $[a,b]$. We compute the polynomial features of both a and b in order to yield the vectors $[1,a,a^2,a^3, ... a^{degree}]$ and $[1,b,b^2,b^3, ... , b^{degree}]$. We train our model with the cartesian product of these polynomial features. The cartesian product generates a new feature vector consisting of all polynomial combinations of the features with degree less than or equal to the specified degree.\n",
    "\n",
    "For example, for degree = 2, we will have the polynomial features $[1,a,a^2]$ and $[1,b,b^2]$ for the datapoint $[a,b]$. The cartesian product of these two vectors will be $[1,a,b,ab,a^2,b^2]$. We do not generate $a^3$ and $b^3$ since their degree is greater than 2 (specified degree).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HmWNcjoplpYO",
    "outputId": "481f70d4-16f7-4d31-9aeb-11ddb364e822"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "POLY_DEGREE = 7\n",
    "N_SAMPLES = 1200\n",
    "\n",
    "rng = np.random.RandomState(seed=10)\n",
    "\n",
    "# Simulating a regression dataset with polynomial features.\n",
    "true_weight = rng.rand(POLY_DEGREE ** 2 + 2, 1)\n",
    "x_feature1 = np.linspace(-5, 5, N_SAMPLES)\n",
    "x_feature2 = np.linspace(-3, 3, N_SAMPLES)\n",
    "x_all = np.stack((x_feature1, x_feature2), axis=1)\n",
    "\n",
    "reg = Regression()\n",
    "x_all_feat = reg.construct_polynomial_feats(x_all, POLY_DEGREE)\n",
    "x_cart_flat = []\n",
    "for i in range(x_all_feat.shape[0]):\n",
    "    point = x_all_feat[i]\n",
    "    x1 = point[:,0]\n",
    "    x2 = point[:,1]\n",
    "    x1_end = x1[-1]\n",
    "    x2_end = x2[-1]\n",
    "    x1 = x1[:-1]\n",
    "    x2 = x2[:-1]\n",
    "    x3 = np.asarray([[m*n for m in x1] for n in x2])\n",
    "\n",
    "    x3_flat = list(np.reshape(x3, (x3.shape[0] ** 2)))\n",
    "    x3_flat.append(x1_end)\n",
    "    x3_flat.append(x2_end)\n",
    "    x3_flat = np.asarray(x3_flat)\n",
    "    x_cart_flat.append(x3_flat)\n",
    "  \n",
    "x_cart_flat = np.asarray(x_cart_flat)\n",
    "x_cart_flat = (x_cart_flat - np.mean(x_cart_flat)) / np.std(x_cart_flat)  # Normalize\n",
    "x_all_feat = np.copy(x_cart_flat)\n",
    "\n",
    "# We must add noise to data, else the data will look unrealistically perfect.\n",
    "y_noise = rng.randn(x_all_feat.shape[0], 1)\n",
    "y_all = np.dot(x_cart_flat, true_weight) + y_noise\n",
    "print(\"x_all: \", x_all.shape[0], \" (rows/samples) \", x_all.shape[1], \" (columns/features)\", sep=\"\")\n",
    "print(\"y_all: \", y_all.shape[0], \" (rows/samples) \", y_all.shape[1], \" (columns/features)\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "Rt_KeaNBlpYO",
    "outputId": "57e09746-c752-4d9a-b5bb-2d51ae666d6e"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "p = np.reshape(np.dot(x_cart_flat, true_weight), (N_SAMPLES,))\n",
    "#ax.plot(x_all[:,0], x_all[:,1], p, label='Line of Best Fit', c=\"red\", linewidth=2)\n",
    "ax.scatter(x_all[:,0], x_all[:,1], y_all, label='Datapoints', s=4)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.legend()\n",
    "ax.text2D(0.05, 0.95, \"All Simulated Datapoints\", transform=ax.transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAeNe7IRlpYO"
   },
   "source": [
    "In the figure above, the red curve is the true fuction we want to learn, while the blue dots are the noisy data points. The data points are generated by  $Y=X\\theta+σ$ , where  σ∼N(0,1)  are i.i.d. generated noise.\n",
    "\n",
    "Now let's split the data into two parts, the training set and testing set. The yellow dots are for training, while the black dots are for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "1-Ov5007lpYP",
    "outputId": "4b20d420-ff27-4f52-d74c-133c50a5ce01"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "PERCENT_TRAIN = 0.5\n",
    "\n",
    "all_indices = rng.permutation(N_SAMPLES)  # Random indicies\n",
    "train_indices = all_indices[:round(N_SAMPLES * PERCENT_TRAIN)]  # 80% Training\n",
    "test_indices = all_indices[round(N_SAMPLES * PERCENT_TRAIN):]  # 20% Testing\n",
    "\n",
    "xtrain = x_all[train_indices]\n",
    "ytrain = y_all[train_indices]\n",
    "xtest = x_all[test_indices]\n",
    "ytest = y_all[test_indices]\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(xtrain[:,0], xtrain[:,1], ytrain, label='Training', c='y',s=4)\n",
    "ax.scatter(xtest[:,0], xtest[:,1], ytest, label='Testing', c='black',s=4)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB2WlAKIlpYP"
   },
   "source": [
    "Now let us train our model using the training set, and see how our model performs on the testing set. Observe the red line, which is our models learn function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKBsoITElpYP",
    "outputId": "312c8796-4ae7-4ef9-a1bf-8444160fee3c"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "weight = reg.linear_fit_closed(x_all_feat[train_indices], y_all[train_indices])\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('Linear (closed) RMSE: %.4f' % test_rmse)\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_pred = np.reshape(y_pred, (y_pred.size,))\n",
    "ax.plot(x_all[:,0], x_all[:,1], y_pred, label='Trendline', color='r', lw=2)\n",
    "\n",
    "ax.scatter(xtrain[:,0], xtrain[:,1], ytrain, label='Training', c='y',s=4)\n",
    "ax.scatter(xtest[:,0], xtest[:,1], ytest, label='Testing', c='black',s=4)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.text2D(0.05, 0.95, \"Linear (Closed)\", transform=ax.transAxes)\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us use our linear gradient descent function with the same setup. Observe that the trendline is now a bit unoptimal and our RMSE increased. Do not be alarmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PXCxdWf3lpYP",
    "outputId": "db4e7947-cd88-4900-952d-57f449015cc8"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "#This cell may take more than 1 minute\n",
    "weight = reg.linear_fit_GD(x_all_feat[train_indices],\n",
    "                           y_all[train_indices],\n",
    "                           epochs=50000,\n",
    "                           learning_rate=1e-8)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "print('Linear (GD) RMSE: %.4f' % test_rmse)\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_pred = np.reshape(y_pred, (y_pred.size,))\n",
    "ax.plot(x_all[:,0], x_all[:,1], y_pred, label='Trendline', color='r', lw=2)\n",
    "\n",
    "ax.scatter(xtrain[:,0], xtrain[:,1], ytrain, label='Training', c='y',s=4)\n",
    "ax.scatter(xtest[:,0], xtest[:,1], ytest, label='Testing', c='black',s=4)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.text2D(0.05, 0.95, \"Linear (GD)\", transform=ax.transAxes)\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must tune our epochs and learning_rate. As we tune these parameters our trendline will approach the trendline generated by the linear closed form solution. Observe how we slowly tune (increase) the epochs and learning_rate below to create a better model.\n",
    "\n",
    "Note that the closed form solution will always give the most optimal/overfit results. We cannot outperform the closed form solution with GD. We can only approach closed forms level of optimality/overfitness. We leave the reasoning behind this as an exercise to the reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "#This cell may take more than 1 minute\n",
    "learning_rates = [1e-8, 1e-6, 1e-4]\n",
    "weights = np.zeros((3, POLY_DEGREE ** 2 + 2))\n",
    "\n",
    "for ii in range(len(learning_rates)):\n",
    "    weights[ii,:] = reg.linear_fit_GD(x_all_feat[train_indices],\n",
    "                                      y_all[train_indices],\n",
    "                                      epochs=50000,\n",
    "                                      learning_rate=learning_rates[ii]).ravel()\n",
    "    y_test_pred = reg.predict(x_all_feat[test_indices],\n",
    "                              weights[ii, :].reshape((POLY_DEGREE ** 2 + 2, 1)))\n",
    "    test_rmse = reg.rmse(y_test_pred, y_all[test_indices])\n",
    "    print('Linear (GD) RMSE: %.4f (learning_rate=%s)' % (test_rmse, learning_rates[ii]))\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "colors = ['g', 'orange', 'r']\n",
    "for ii in range(len(learning_rates)):\n",
    "    y_pred = reg.predict(x_all_feat, weights[ii])\n",
    "    y_pred = np.reshape(y_pred, (y_pred.size,))\n",
    "    ax.plot(x_all[:,0], x_all[:,1], y_pred,\n",
    "            label='Trendline LR=' + str(learning_rates[ii]),\n",
    "            color=colors[ii], lw=2)\n",
    "\n",
    "ax.scatter(xtrain[:,0], xtrain[:,1], ytrain, label='Training', c='y',s=4)\n",
    "ax.scatter(xtest[:,0], xtest[:,1], ytest, label='Testing', c='black',s=4)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.text2D(0.05, 0.95, \"Tunning Linear (GD)\", transform=ax.transAxes)\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vR6MVOBKlpYP"
   },
   "source": [
    "And what if we just use the first 10 data points to train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pduyAUn8lpYP",
    "outputId": "710050e2-e45d-435c-fd47-d19da05259ef"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "rng = np.random.RandomState(seed=5)\n",
    "y_all_noisy = np.dot(x_cart_flat, np.zeros((POLY_DEGREE ** 2 + 2, 1))) + rng.randn(x_all_feat.shape[0], 1)\n",
    "sub_train = train_indices[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "dh5wDvzNlpYQ",
    "outputId": "115c8c5b-4964-4ec4-d06b-a2d148285ab9"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "weight = reg.linear_fit_closed(x_all_feat[sub_train], y_all_noisy[sub_train])\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print('Linear (closed) 10 Samples RMSE: %.4f' % test_rmse)\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x1 = x_all[:,0]\n",
    "x2 = x_all[:,1]\n",
    "y_pred = np.reshape(y_pred, (N_SAMPLES,))\n",
    "ax.plot(x1, x2, y_pred, color='b', lw=4)\n",
    "\n",
    "x3 = x_all[sub_train,0]\n",
    "x4 = x_all[sub_train,1]\n",
    "ax.scatter(x3, x4, y_all_noisy[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.set_zlim([None, 8])\n",
    "ax.text2D(0.05, 0.95, \"Linear Regression (Closed)\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaS3FcNJlpYP"
   },
   "source": [
    "Did you see a worse performance? Let's take a closer look at what we have learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anXODlZmlpYQ"
   },
   "source": [
    "### 3.4 Testing: Testing ridge regression [5 pts] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvfvWd0olpYQ"
   },
   "source": [
    "\n",
    "\n",
    "Now let's try ridge regression. Similarly, undergraduate students need to implement the closed form, and graduate students need to implement all the three methods. We will call the prediction function from linear regression part. As long as your test rmse score is close to the TA's answer (TA's answer $\\pm 0.5$), you can get full points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9cRpMWWlpYQ"
   },
   "source": [
    "Again, let's see what we have learned. You only need to run the cell corresponding to your specific implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "Zze3sPKIlpYQ",
    "outputId": "20a228dd-08bf-4e10-8d5c-ae0b59224fac"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "weight = reg.ridge_fit_closed(x_all_feat[sub_train],\n",
    "                              y_all_noisy[sub_train],\n",
    "                              c_lambda=10)\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print('Ridge Regression (closed) RMSE: %.4f' % test_rmse)\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x1 = x_all[:,0]\n",
    "x2 = x_all[:,1]\n",
    "y_pred = np.reshape(y_pred, (N_SAMPLES,))\n",
    "ax.plot(x1, x2, y_pred, color='b', lw=4)\n",
    "\n",
    "x3 = x_all[sub_train,0]\n",
    "x4 = x_all[sub_train,1]\n",
    "ax.scatter(x3, x4, y_all_noisy[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.set_zlim([None, 8])\n",
    "ax.text2D(0.05, 0.95, \"Ridge Regression (Closed)\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "bY7DvrZPlpYQ",
    "outputId": "953cf781-398b-47c4-acec-2ffff6aa7665"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "weight = reg.ridge_fit_GD(x_all_feat[sub_train],\n",
    "                          y_all_noisy[sub_train],\n",
    "                          c_lambda=10, learning_rate=1e-5)\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print('Ridge Regression (GD) RMSE: %.4f' % test_rmse)\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x1 = x_all[:,0]\n",
    "x2 = x_all[:,1]\n",
    "y_pred = np.reshape(y_pred, (N_SAMPLES,))\n",
    "ax.plot(x1, x2, y_pred, color='b', lw=4)\n",
    "\n",
    "x3 = x_all[sub_train,0]\n",
    "x4 = x_all[sub_train,1]\n",
    "ax.scatter(x3, x4, y_all_noisy[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.set_zlim([None, 8])\n",
    "ax.text2D(0.05, 0.95, \"Ridge Regression (GD)\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "eOCxI-a5lpYQ",
    "outputId": "1ffc9f73-b1d0-4796-a353-25011ac89c0b"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "weight = reg.ridge_fit_SGD(x_all_feat[sub_train],\n",
    "                           y_all_noisy[sub_train],\n",
    "                           c_lambda=10,\n",
    "                           learning_rate=1e-5)\n",
    "y_pred = reg.predict(x_all_feat, weight)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print('Ridge Regression (SGD) RMSE: %.4f' % test_rmse)\n",
    "\n",
    "\n",
    "# -- Plotting Code --\n",
    "fig = plt.figure(figsize=(8,5), dpi=120)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x1 = x_all[:,0]\n",
    "x2 = x_all[:,1]\n",
    "y_pred = np.reshape(y_pred, (N_SAMPLES,))\n",
    "ax.plot(x1, x2, y_pred, color='b', lw=4)\n",
    "\n",
    "x3 = x_all[sub_train,0]\n",
    "x4 = x_all[sub_train,1]\n",
    "ax.scatter(x3, x4, y_all_noisy[sub_train], s=100, c='r', marker='x')\n",
    "\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "ax.set_xlabel(\"feature 1\")\n",
    "ax.set_ylabel(\"feature 2\")\n",
    "ax.set_zlabel(\"y\")\n",
    "ax.set_zlim([None, 8])\n",
    "ax.text2D(0.05, 0.95, \"Ridge Regression (SGD)\", transform=ax.transAxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbfhyxI_lpYQ"
   },
   "source": [
    "### 3.5 Cross validation [7 pts] <span style=\"color:green\">**[W]**</span>\n",
    "Let's use Cross Validation to find the best value for c_lambda in ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ShOhnTvvlpYR",
    "outputId": "2cf256e0-4eac-4156-ae18-7441396a6f9b"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "# We provided 6 possible values for lambda, and you will use them in cross validation.\n",
    "# For cross validation, use 10-fold method and only use it for your training data (you already have the train_indices to get training data).\n",
    "# For the training data, split them in 10 folds which means that use 10 percent of training data for test and 90 percent for training.\n",
    "# At the end for each lambda, you have caluclated 10 rmse and get the mean value of that.\n",
    "# That's it. Pick up the lambda with the lowest mean value of rmse. \n",
    "# Hint: np.concatenate is your friend.\n",
    "best_lambda = None\n",
    "best_error = None\n",
    "kfold = 10\n",
    "lambda_list = [0.0001, 0.001, 0.1, 1, 5, 10, 50, 100, 1000, 10000]\n",
    "\n",
    "for lm in lambda_list:\n",
    "    err = reg.ridge_cross_validation(x_all_feat[train_indices], y_all[train_indices], kfold, lm)\n",
    "    print('Lambda: %.4f' % lm, 'RMSE: %.6f'% err)\n",
    "    if best_error is None or err < best_error:\n",
    "        best_error = err\n",
    "        best_lambda = lm\n",
    "\n",
    "print('Best Lambda: %.4f' % best_lambda)\n",
    "weight = reg.ridge_fit_closed(x_all_feat[train_indices], y_all_noisy[train_indices], c_lambda=10)\n",
    "y_test_pred = reg.predict(x_all_feat[test_indices], weight)\n",
    "test_rmse = reg.rmse(y_test_pred, y_all_noisy[test_indices])\n",
    "print('Best Test RMSE: %.4f' % test_rmse)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4z1pZeSlpYR"
   },
   "source": [
    "### 3.6 Noisy Input Samples in Linear Regression [10 pts] <span style=\"color:green\">**[W]**</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_i9qEyuMEbf"
   },
   "source": [
    "Consider a linear model of the form:\n",
    "$$\n",
    "y(x_n,\\theta) = \\theta_0 + \\sum_{d=1}^D\\theta_dx_{nd}\n",
    "$$\n",
    "where $x_n = (x_{n1},...,x_{nD})$ and weights $\\theta = (\\theta_0,...,\\theta_D)$. Given the the D-dimension input sample set $x = \\{ x_1,...,x_n\\}$ with corresponding target value $y = \\{y_1,...,y_n\\}$, the sum-of-squares error function is:\n",
    "$$\n",
    "E_D(\\theta) = \\frac{1}{2}\\sum_{n=1}^N\\left\\{y(x_n,\\theta)-y_n\\right\\}^2\n",
    "$$\n",
    "\n",
    "Now, suppose that Gaussian noise $\\epsilon_n$ with zero mean and variance $\\sigma^2$ is added independently to each of the input sample $x_n$ to generate a new sample set $x'= \\{x_1+\\epsilon_1,...,x_n+\\epsilon_n\\}$. For each sample $x_n$, $x_n' = (x_{n1} + \\epsilon_{n1},...,x_{nD} + \\epsilon_{nd})$, where $n$ and $d$ is independent across both $n$ and $d$ indices. \n",
    "\n",
    "1. (3pts) Show that $y(x_n',\\theta) = y(x_n, \\theta) + \\sum^D_{d=1}\\theta_d\\epsilon_{nd}$ \n",
    "\n",
    "2. (7pts) Assume the sum-of-squares error function of the noise sample set $x'= \\{x_1+\\epsilon_1,...,x_n+\\epsilon_n\\}$ is $E_D(\\theta)'$. Prove the expectation of $E_D(\\theta)'$ is equivalent to the sum-of-squares error $E_D(\\theta)$ for noise-free input samples with the addition of a weight-decay regularization term  (e.g. $L_2$ norm) , in which the bias parameter $\\theta_0$ is omitted from the regularizer. In other words, show that\n",
    "$$\n",
    "E[E_D(\\theta)'] = E_D(\\theta) + regularizer\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYZ4uDIqMIFM",
    "tags": []
   },
   "source": [
    "**Hint:** \n",
    "\n",
    "\n",
    "*   During the class, we have discussed how to solve for the weight $\\theta$ for ridge regression, the function looks like this: \n",
    "$$\n",
    "E(\\theta)=\\frac{1}{N}\\sum_{i=1}^N\\left\\{ y(x_i,\\theta)-y_i \\right\\}^2+\\frac{\\lambda}{N}\\sum_{i=1}^d ||\\theta_i||^2\n",
    "$$\n",
    "where the first term is the sum-of-squares error and the second term is the regularization term. N is the number of samples. In this question, we use another form of the ridge regression, which is:\n",
    "$$\n",
    "E(\\theta)=\\frac{1}{2}\\sum_{i=1}^N\\left\\{ y(x_i,\\theta)-y_i \\right\\}^2+\\frac{\\lambda}{2}\\sum_{i=1}^d ||\\theta_i||^2\n",
    "$$\n",
    "* For the Gaussian noise $\\epsilon_n$, we have $E[\\epsilon_n]=0$\n",
    "\n",
    "*  Assume the noise $\\epsilon = (\\epsilon_1,..., \\epsilon_n)$ are **independent** to each other, we have\n",
    "$$ \n",
    "E[\\epsilon_n\\epsilon_m]=\\left\\{\n",
    "\\begin{array}{rcl}\n",
    "\\sigma^2       &      & m = n\\\\\n",
    "0     &      & m \\neq n\\\\\n",
    "\\end{array} \\right. \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Q4: Naive Bayes Classification [25pts] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Naive Bayes in Marketing [5pts] <span style=\"color:green\">[W]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mia, a marketer from the local movie theatre company *Dank ML Memes*, wants to evaluate the demand of the new box office opening of *Mahdi and the Memes*. She sampled 12 customers randomly and conducted a survey to learn about their lifestyles. The table below shows the chance of the customer attending the opening and their lifestyle.\n",
    "\n",
    "| Chance of Attending the Box Office Opening | Does the customer have a loyalty plan? | Movie Watching Frequency (days/wk) | Average memes viewed per day | Favorite Meme (out of 3 options) |\n",
    "| :----------------------------------------: | :------------------------------------: | :--------------------------------: | :--------------------------: | :------------------------------: |\n",
    "|                    High                    |                  Yes                   |                0-3                 |              <6              |         Expanding Brain          |\n",
    "|                   Medium                   |                  Yes                   |                0-3                 |             6-9              |        Surprised Pikachu         |\n",
    "|                    Low                     |                  Yes                   |                 >3                 |             6-9              |      Crying Michael Jordan       |\n",
    "|                   Medium                   |                   No                   |                 >3                 |             6-9              |         Expanding Brain          |\n",
    "|                    Low                     |                   No                   |                0-3                 |             6-9              |         Expanding Brain          |\n",
    "|                    Low                     |                   No                   |                 >3                 |              <6              |        Surprised Pikachu         |\n",
    "|                    High                    |                  Yes                   |                 >3                 |              <6              |        Surprised Pikachu         |\n",
    "|                   Medium                   |                   No                   |                 >3                 |              <6              |      Crying Michael Jordan       |\n",
    "|                    High                    |                   No                   |                0-3                 |             6-9              |      Crying Michael Jordan       |\n",
    "|                    Low                     |                   No                   |                0-3                 |             6-9              |        Surprised Pikachu         |\n",
    "|                   Medium                   |                  Yes                   |                0-3                 |             6-9              |      Crying Michael Jordan       |\n",
    "|                    Low                     |                  Yes                   |                 >3                 |             6-9              |      Crying Michael Jordan       |\n",
    "\n",
    "Given that a customer has a loyalty plan who watches movies >3 days/wk, views 6-9 memes on a daily average, and has the Surprised Pikachu as their favorite meme, assess the chance this person is attending the Box Office Opening using Naive Bayes.\n",
    "\n",
    "**Note:** You can assume that each habit of a person is independent from other habits i.e. A person who watches movies regularly does not tell any information about his/her meme-viewing pattern or whether he/she has a loyalty plan, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Determining Amazon Product Ratings from Product Reviews [15pts] <span style=\"color:blue\">**[P]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Budd was recently hired by the Amazon to analyze product reviews from select Luxury Beauty Products to determine political product rating (ex: 1-star, 2-star, 3-star, etc.). Budd, a skilled CS4641/7641 alumnus himself, decides to use a Naive Bayes approach to classify Amazon product reviews.\n",
    "\n",
    "The original dataset has 5 classes: 1-star (class label = 1), 2-star (class label = 2), 3-star (class label = 3), 4-star (class label = 4), 5-star (class label = 5). However, Brian is interested to see how Naive Bayes would perform when he groups these original labels together. He decides to make a 2-label, 3-label, 4-label, and 5-label (which is the original dataset) Naive Bayes models. There are over 200,000 product review. However, to save computational time as well as memory resources, the dataset has been reduced to 40,000 unique product reviews. These product reviews have also been cleaned to remove extra spaces, punctuation, emojis, etc. The dataset (which remains the same except for the number of labels) is then split into a training and testing dataset that has a 8:2 ratio.\n",
    "\n",
    "The code which is provided loads the product reviews and builds a [“bag of words” representation](https://en.wikipedia.org/wiki/Bag-of-words_model) of each product review. Your task is to complete the missing portions of the code and to determine what the star-rating was given for that product review.\n",
    "\n",
    "**The function explanations below are explained assuming the 5-label model.**\n",
    "\n",
    "**priors_prob** function calculates the ratio of class probabilities of 1-star, 2-star, 3-star, 4-star, and 5-star. We do this based on word counts rather than document counts.\n",
    "\n",
    "**likelihood_ratio** function calculates the ratio of word probablities given the label of whether the star rating was 1-star, 2-star, 3-star, 4-star, and 5-star\n",
    "\n",
    "**analyze_star_rating** function takes in the likelihood ratio, priors probabilities for each class and a number of test product reviews represented in Bag-of-Words representation, and analyzes the star-rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we have a matrix like: (the first column denotes the class label, the entries in the remaining columns denote the number of occurrances for each word). We have two more columns for words. The first word is \"machine\" and the second word is \"learning\"\n",
    "\n",
    "**For this example, we will be assuming the 2-label model**\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} \n",
    "    \\text{label} & \\text{machine} & \\text{learning} \\\\ \n",
    "    0(\\text{rating} \\leq 2) & 1 & 4 \\\\ \n",
    "    0 & 0 & 6 \\\\ \n",
    "    1(\\text{rating} \\geq 3) & 3 & 2 \\\\\n",
    "    0 & 3 & 1 \\\\\n",
    "    1 & 4 & 0\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Then we have \n",
    "$$prior(\\text{rating} \\leq 2) = \\frac{1+4+0+6+3+1}{1+4+0+6+3+2+3+1+4+0} = \\frac{15}{24}$$\n",
    "$$prior(\\text{rating} \\geq 3) = \\frac{3+2+4+0}{1+4+0+6+3+2+3+1+4+0} = \\frac{9}{24}$$\n",
    "\n",
    "**Note 1:** In likelihood_ratio(), add one to each word count so as to avoid issues with zero word count. This is known as Add-1 smoothing. It is a type of additive smoothing. For the numerator, we just add 1 at the end. For the denominator, we add 1 for each feature (in this example, for each word).\n",
    "\n",
    "$$likelihood(\\text{rating} \\leq 2) = [\\frac{1+0+3+\\mathbf{1}}{1+0+3+\\mathbf{1}+4+6+1+\\mathbf{1}}\\ \\frac{4+6+1+\\mathbf{1}}{1+0+3+\\mathbf{1}+4+6+1+\\mathbf{1}}] = [\\frac{5}{17}\\ \\frac{12}{17}]$$\n",
    "$$likelihood(\\text{rating} \\geq 3) = [\\frac{3+4+\\mathbf{1}}{3+4+\\mathbf{1}+2+0+\\mathbf{1}}\\ \\frac{2+0+\\mathbf{1}}{3+4+\\mathbf{1}+2+0+\\mathbf{1}}] = [\\frac{8}{11}\\ \\frac{3}{11}]$$\n",
    "\n",
    "**Note 2:** In analyze_affiliation(), we can calculate the posterior probability given the count for each word\n",
    "<center>\n",
    "<figure><table>\n",
    "<thead>\n",
    "<tr><th>&nbsp;</th><th>Machine</th><th>Learning</th></tr></thead>\n",
    "<tbody><tr><td>Count</td><td>3</td><td>4</td></tr></tbody>\n",
    "</table></figure>\n",
    "<p>&nbsp;</p>\n",
    "</center>\n",
    "\n",
    "$$\n",
    "P(\\text{rating} \\leq 2) = (\\frac{5}{17})^3 * (\\frac{12}{17})^4 * \\frac{15}{24}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{rating} \\geq 3) = (\\frac{8}{11})^3 * (\\frac{3}{11})^4 * \\frac{9}{24}\n",
    "$$\n",
    "\n",
    "The prediction will then be the label with the highest probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from nb import NaiveBayes\n",
    "import preprocessor as p  #if this command errors, please pip install tweet-preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    tweet = p.clean(tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    return tweet\n",
    "\n",
    "def assign_labels(train_dataset, class_to_label_mappings, vectorizer):\n",
    "    new_train = train_dataset.copy()\n",
    "    new_train[\"overall\"] = new_train[\"overall\"].map(class_to_label_mappings)\n",
    "    X = new_train['summary'].values\n",
    "    y = new_train['overall'].values\n",
    "    BOW = vectorizer.fit_transform(X).toarray()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(BOW, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "def build_and_test_model(X_train, y_train, X_test, y_test):\n",
    "    list_of_labels = [X_train[y_train == label] for label in np.unique(y_train)]\n",
    "    likelihood_ratio = NB.likelihood_ratio(list_of_labels)\n",
    "    priors_prob = NB.priors_prob(list_of_labels)\n",
    "    resolved = NB.analyze_star_rating(likelihood_ratio, priors_prob, X_test)\n",
    "    return np.sum(resolved == y_test) / len(resolved) * 1.\n",
    "\n",
    "RANDOM_SEED = 5\n",
    "\n",
    "# Source: https://nijianmo.github.io/amazon/index.html\n",
    "print(\"Opening Dataset...\")\n",
    "data = []\n",
    "with gzip.open('./data/Luxury_Beauty_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "        \n",
    "print(\"Preprocessing Dataset...\")        \n",
    "train = pd.DataFrame.from_dict(data)\n",
    "train = train.fillna('')\n",
    "\n",
    "train = train.drop(columns=[column for column in train.columns if column != 'overall' and column != 'summary'])\n",
    "train['overall'] = train['overall'].astype('int8')\n",
    "\n",
    "pbar = tqdm(total=train.shape[0])\n",
    "for _, row in train.iterrows():\n",
    "    row['summary'] = clean_tweet(row['summary'])\n",
    "    pbar.update(1)\n",
    "train.drop_duplicates(inplace=True)\n",
    "\n",
    "class_to_label_2 = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 1,\n",
    "    5: 1\n",
    "}\n",
    "\n",
    "class_to_label_3 = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2\n",
    "}\n",
    "\n",
    "class_to_label_4 = {\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 3\n",
    "}\n",
    "\n",
    "class_to_label_5 = {\n",
    "    1: 1,\n",
    "    2: 2,\n",
    "    3: 3,\n",
    "    4: 4,\n",
    "    5: 5\n",
    "}\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "vectorizer = text.CountVectorizer(stop_words=stop_words)\n",
    "\n",
    "print(\"Assigning labels...\")\n",
    "X_train_2, y_train_2, X_test_2, y_test_2 = assign_labels(train, class_to_label_2, vectorizer)\n",
    "X_train_3, y_train_3, X_test_3, y_test_3 = assign_labels(train, class_to_label_3, vectorizer)\n",
    "X_train_4, y_train_4, X_test_4, y_test_4 = assign_labels(train, class_to_label_4, vectorizer)\n",
    "X_train_5, y_train_5, X_test_5, y_test_5 = assign_labels(train, class_to_label_5, vectorizer)\n",
    "\n",
    "print(\"Building and testing models...\")\n",
    "NB = NaiveBayes()\n",
    "accuracy_2 = build_and_test_model(X_train_2, y_train_2, X_test_2, y_test_2)\n",
    "accuracy_3 = build_and_test_model(X_train_3, y_train_3, X_test_3, y_test_3)\n",
    "accuracy_4 = build_and_test_model(X_train_4, y_train_4, X_test_4, y_test_4)\n",
    "accuracy_5 = build_and_test_model(X_train_5, y_train_5, X_test_5, y_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Should be 91%\n",
    "print(round(accuracy_2 * 100, 3))\n",
    "\n",
    "# Should be 78%\n",
    "print(round(accuracy_3 * 100, 3))\n",
    "\n",
    "# Should be 54%\n",
    "print(round(accuracy_4 * 100, 3))\n",
    "\n",
    "# Should be 22%\n",
    "print(round(accuracy_5 * 100, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Accuracy result analysis [5pts] <span style=\"color:green\">**[W]**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the trend between accuracy and number of labels? Why do you think this is the case? What assumptions can you make that limit the accuracy? (This is an open question, any reasonable assumptions will be acceptable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nOSw42MOJcv"
   },
   "source": [
    "## Q5: Noise in PCA and Linear Regression\n",
    "\n",
    "Both PCA and least squares regression can be viewed as algorithms for inferring (linear) relationships\n",
    "among data variables. In this part of the assignment, you will develop some intuition for the differences\n",
    "between these two approaches, and an understanding of the settings that are better suited to using PCA or\n",
    "better suited to using the least squares fit.\n",
    "\n",
    "The high level bit is that PCA is useful when there is a set of latent (hidden/underlying)\n",
    "variables, and all the coordinates of your data are linear combinations (plus noise) of those variables. The\n",
    "least squares fit is useful when you have direct access to the independent variables, so any noisy coordinates\n",
    "are linear combinations (plus noise) of known variables.\n",
    "\n",
    "### 5.1 Slope Functions (5 Pts) <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "In the **following cell**, complete the following:\n",
    "1. **pca_slope**: For this function, assume that X is the first feature and Y is the second feature for the data. Write a function, that takes in the first feature vector X and the second feature vector Y. Stack these two feature vectors into a single Nx2 matrix and use this to determine the first principal component vector of this dataset. Finally, return the slope of this first component. You should use the PCA implementation from Q2. <br /><br />\n",
    "\n",
    "2. **lr_slope**: Write a function that takes X and y and returns the slope of the least squares fit. You should use the Linear Regression implementation from Q3 but do not use any kind of regularization. Think about how weight could relate to slope. \n",
    "\n",
    "In later subparts, we consider the case where our data consists of noisy measurements of x and y. For each part, we will evaluate the quality of the relationship recovered by PCA, and that recovered by standard least\n",
    "squares regression.\n",
    "\n",
    "As a reminder, least squares regression minimizes the squared error of the dependent variable from its prediction. Namely, given $(x_i, y_i)$ pairs, least squares returns the line $l(x)$ that minimizes $\\sum_i (y_i − l(x_i))^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw-PRRnWOJcw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pca import PCA\n",
    "from regression import Regression\n",
    "\n",
    "def pca_slope(X, y):\n",
    "    \"\"\"\n",
    "    Calculates the slope of the first principal component given by PCA\n",
    "\n",
    "    Args: \n",
    "        x: (N,) vector of feature x\n",
    "        y: (N,) vector of feature y\n",
    "    Return:\n",
    "        slope: Scalar slope of the first principal component\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def lr_slope(X, y):\n",
    "    \"\"\"\n",
    "    Calculates the slope of the best fit as given by Linear Regression\n",
    "\n",
    "    For this function don't use any regularization\n",
    "\n",
    "    Args: \n",
    "        X: N*1 array corresponding to a dataset\n",
    "        y: N*1 array of labels y\n",
    "    Return:\n",
    "        slope: Scalar slope of the best fit\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider a simple example with two variables, x and y, where the true relationship between the\n",
    "variables is y = 5x. Our goal is to recover this relationship—namely, recover the coefficient “5”. We set X = [0, .02, .04, .06, . . . , 1] and y = 5x. Make sure both functions return 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "x = np.arange(0, 1.02, 0.02)\n",
    "\n",
    "y = 5 * np.arange(0, 1.02, 0.02)\n",
    "\n",
    "print(\"Slope of first principal component\", pca_slope(x, y))\n",
    "\n",
    "print(\"Slope of best linear fit\", lr_slope(x[:, None], y))\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Analysis Setup (5 Pts) <span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "### Error in y\n",
    "\n",
    "In this subpart, we consider the setting where our data consists of the actual values of $x$, and noisy estimates of $y$. Run the following cell to see how the data looks when there is error in $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "base = np.arange(0.001, 1.001, 0.001)\n",
    "c = 0.5\n",
    "X = base\n",
    "y = 5 * base + np.random.normal(loc=[0], scale=c, size=base.shape)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In <strong>following cell</strong>, you will implement the <strong>addNoise</strong> function:\n",
    "1. Create a vector $X$ where $X = [x_1, x_2, . . . , x_{1000}] = [.001, .002, .003, . . . , 1]$.<br /><br />\n",
    "2. For a given noise level $c$, set $ \\hat{y}_i ∼ 5x_i + \\mathcal{N}(0, c) = 5i/1000 + \\mathcal{N}(0, c)$, and $\\hat{Y} = [\\hat{y}_1, \\hat{y}_2, . . . , \\hat{y}_{1000}]$. You can use the np.random.normal function, where scale is equal to noise level, to add noise to your points.<br /><br />\n",
    "3. Notice the parameter <strong>x_noise</strong> in the <strong>addNoise</strong> function. When this parameter is set to $True$, you will have to add noise to $X$. For a given noise level c, let $\\hat{x}_i ∼ x_i + \\mathcal{N}(0, c) = i/1000 + \\mathcal{N}(0, c)$, and $\\hat{X} = [\\hat{x}_1, \\hat{x}_2, . . . . \\hat{x}_{1000}]$ <br /><br />\n",
    "4. Return the <strong>pca_slope</strong> and <strong>lr_slope</strong> values of this $X$ and $\\hat{Y}$ dataset you have created where $\\hat{Y}$ has noise ($X = X$ or $\\hat{X}$ depending on the problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNoise(c, x_noise = False, seed = 1):\n",
    "    \"\"\"\n",
    "    Creates a dataset with noise and calculates the slope of the dataset\n",
    "    using the pca_slope and lr_slope functions implemented in this class.\n",
    "\n",
    "    Args: \n",
    "        c: Scalar, a given noise level to be used on Y and/or X\n",
    "        x_noise: Boolean. When set to False, X should not have noise added\n",
    "                 When set to True, X should have noise\n",
    "        seed: Random seed\n",
    "    Return:\n",
    "        pca_slope_value: slope value of dataset created using pca_slope\n",
    "        lr_slope_value: slope value of dataset created using lr_slope\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(seed) #### DO NOT CHANGE THIS ####\n",
    "    ############# START YOUR CODE BELOW #############\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return pca_slope_value, lr_slope_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scatter plot with c on the horizontal axis, and the output of <strong>pca_slope</strong> and <strong>lr_slope</strong> on the vertical axis has already been implemented for you.\n",
    "\n",
    "A sample $\\hat{Y}$ has been taken for each $c$ in $[0, 0.05, 0.1, . . . , .95, 1.0]$. The output of <strong>pca_slope</strong> is plotted as a red dot, and the output of <strong>lr_slope</strong> as a blue dot. This has been repeated 30 times, you can see that we end up with a plot of 1260 dots, in 21 columns of 60, half red and half blue.\n",
    "\n",
    "**Note**: Our x_noise = False since we only want Y to have any noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "pca_slope_values = []\n",
    "linreg_slope_values = []\n",
    "c_values = []\n",
    "s_idx = 0\n",
    "\n",
    "for i in range(30):\n",
    "    for c in np.arange(0, 1.05, 0.05):\n",
    "        \n",
    "        # Calculate pca_slope_value (psv) and lr_slope_value (lsv)\n",
    "        psv, lsv = addNoise(c, seed = s_idx)\n",
    "        \n",
    "        # Append pca and lr slope values to list for plot function\n",
    "        pca_slope_values.append(psv)\n",
    "        linreg_slope_values.append(lsv)\n",
    "        \n",
    "        # Append c value to list for plot function\n",
    "        c_values.append(c)\n",
    "        \n",
    "        # Increment random seed index\n",
    "        s_idx += 1\n",
    "        \n",
    "plt.scatter(c_values, pca_slope_values, c='r')\n",
    "plt.scatter(c_values, linreg_slope_values, c='b')\n",
    "plt.xlabel(\"c\")\n",
    "plt.ylabel(\"slope\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error in x and y<span style=\"color:green\">**[W]**</span>\n",
    "\n",
    "We will now examine the case where our data consists of noisy estimates of <strong>both</strong> $x$ and $y$. Run the following cell to see how the data looks when there is error in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "base = np.arange(0.001, 1, 0.001)\n",
    "c = 0.5\n",
    "X = base + np.random.normal(loc=[0], scale=c, size=base.shape)\n",
    "y = 5 * base + np.random.normal(loc=[0], scale=c, size=base.shape)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below cell, we graph the predicted PCA and LR slopes on the vertical axis against the value of c on the horizontal axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "pca_slope_values = []\n",
    "linreg_slope_values = []\n",
    "c_values = []\n",
    "s_idx = 0\n",
    "\n",
    "for i in range(30):\n",
    "    for c in np.arange(0, 1.05, 0.05):\n",
    "        \n",
    "        # Calculate pca_slope_value (psv) and lr_slope_value (lsv), notice x_noise = True\n",
    "        psv, lsv = addNoise(c, x_noise = True, seed = s_idx)\n",
    "        \n",
    "        # Append pca and lr slope values to list for plot function\n",
    "        pca_slope_values.append(psv)\n",
    "        linreg_slope_values.append(lsv)\n",
    "        \n",
    "        # Append c value to list for plot function\n",
    "        c_values.append(c)\n",
    "        \n",
    "        # Increment random seed index\n",
    "        s_idx += 1\n",
    "\n",
    "plt.scatter(c_values, pca_slope_values, c='r')\n",
    "plt.scatter(c_values, linreg_slope_values, c='b')\n",
    "plt.xlabel(\"c\")\n",
    "plt.ylabel(\"slope\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Analysis (5 Pts) <span style=\"color:green\">**[W]**</span>\n",
    "Based on your observations from previous subsections answer the following questions about the two cases (error in $Y$ and error in both $X$ and $Y$) in 2-3 lines. \n",
    "\n",
    "Note: \n",
    "1. The closer the value of slope to actual slope (\"5\" here) the better the algorithm is performing.\n",
    "2. You don't need to provide a mathematical proof for this question.\n",
    "\n",
    "Questions:\n",
    "1. Which case does PCA perform worse in? Why does PCA perform worse in this case? (2 Pts)\n",
    "2. Why does PCA perform better in the other case? (1 Pt)\n",
    "3. Which case does Linear Regression perform well? Why does Linear Regression perform well in this case? (2 Pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.datasets import load_boston, load_diabetes, load_digits, load_breast_cancer, load_iris, load_wine\n",
    "from sklearn.linear_model import Ridge, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.csgraph import floyd_warshall\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Feature Reduction Implementation [25 Points Bonus for All] [P + W]\n",
    "\n",
    "### 6.1 Implementation [18 Points] [P]\n",
    "\n",
    "Feature selection is an integral aspect of machine learning. It is the process of selecting a subset of relevant features that are to be used as the input for the machine learning task. Feature selection may lead to simpler models for easier interpretation, shorter training times, avoidance of the curse of dimensionality, and better generalization by reducing overfitting.\n",
    "\n",
    "\n",
    "Implement a method to find the final list of significant features due to forward selection and backward elimination.\n",
    "\n",
    "Forward Selection:\n",
    "\n",
    "In forward selection, we start with a null model, start fitting the model with one individual feature at a time, and select the feature with the minimum p-value. We continue to do this until we have a set of features where one feature's p-value is less than the confidence level.\n",
    "\n",
    "Steps to implement it:\n",
    "\n",
    "1. Choose a significance level (given to you).\n",
    "2. Fit all possible simple regression models by considering one feature at a time.\n",
    "3. Select the feature with the lowest p-value.\n",
    "4. Fit all possible models with one extra feature added to the previously selected feature(s).\n",
    "5. Select the feature with the minimum p-value again. if p_value < significance, go to Step 4. Otherwise, terminate.\n",
    "Backward Elimination:\n",
    "\n",
    "In backward elimination, we start with a full model, and then remove the insignificant feature with the highest p-value (that is greater than the significance level). We continue to do this until we have a final set of significant features.\n",
    "\n",
    "Steps to implement it:\n",
    "\n",
    "1. Choose a significance level (given to you).\n",
    "2. Fit a full model including all the features.\n",
    "3. Select the feature with the highest p-value. If (p_value > significance level), go to Step 4, otherwise terminate.\n",
    "4. Remove the feature under consideration.\n",
    "5. Fit a model without this feature. Repeat entire process from Step 3 onwards.\n",
    "TIP 1: The p-value is known as the observed significance value for a test hypothesis. It tests all the assumptions about how the data was generated in the model, not just the target hypothesis it was supposed to test. Some more information about p-values can be found here: https://towardsdatascience.com/what-is-a-p-value-b9e6c207247f\n",
    "\n",
    "TIP 2: For this function, you will have to install statsmodels if not installed already. Run 'pip install statsmodels' in command line/terminal. In the case that you are using an Anaconda environment, run 'conda install -c conda-forge statsmodels' in the command line/terminal. For more information about installation, refer to https://www.statsmodels.org/stable/install.html. The statsmodels library is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. You will have to use this library to choose a regression model to fit your data against. Some more information about this module can be found here: https://www.statsmodels.org/stable/index.html\n",
    "\n",
    "TIP 3: For step 2 in each of the forward and backward selection functions, you can use the 'sm.OLS' function as your regression model. Also, do not forget to add a bias to your regression model. A function that may help you is the 'sm.add_constants' function.\n",
    "\n",
    "TIP 4: You should be able to implement these function using only the libraries provided in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from feature_reduction import FeatureReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "wine = load_wine()\n",
    "win = pd.DataFrame(wine.data, columns = wine.feature_names)\n",
    "win['Price'] = wine.target\n",
    "X = win.drop(\"Price\", 1)       # feature matrix \n",
    "y = win['Price']               # target feature\n",
    "featurereduction = FeatureReduction()\n",
    "#Run the functions to make sure two lists are generated, one for each method\n",
    "print(\"Features selected by forward selection:\", featurereduction.forward_selection(X, y))\n",
    "print(\"Features selected by backward selection:\", featurereduction.backward_elimination(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6.2 Feature Selection - Discussion [7pts] [W]\n",
    "\n",
    "From 6.1, we see two methodologies of feature selection: forward selction and backward elimination. Specifically the process of adding or removing features one-by-one into a model is considered stepwise selection. What are the advantages and disadvantages of using stepwise selection? Under what situations is forward selection more advantageous to use than backward elimination? Under what situations is backward elimination more advantageous to use than forward selection? (7 pts)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_Template.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "2db524e06e9f5f4ffedc911c917cb75e12dbc923643829bf417064a77eb14d37"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
