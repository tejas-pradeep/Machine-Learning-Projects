{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fall 2021 CS4641/CS7641 A Homework 2\n",
    "\n",
    "## Instructor: Dr. Mahdi Roozbahani\n",
    "\n",
    "## Deadline: October 19th, Tuesday, 11:59 pm AOE\n",
    "\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "* Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "<font color='darkred'>\n",
    "* Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own. </font>\n",
    "<font color='darkred'>\n",
    "* All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the instituteâ€™s Academic Integrity procedures (e.g., reported to and directly handled by the Office of Student Integrity (OSI)). **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    " </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for the assignment \n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- This assignment consists of both programming and theory questions.\n",
    "\n",
    "- To switch between cell for code and for markdown, see the menu -> Cell -> Cell Type\n",
    "    \n",
    "- You can directly type Latex equations into markdown cells.\n",
    "    \n",
    "- If a question requires a picture, you could use this syntax $\"<img src=\"\" style=\"width: 300px;\"/>\"$ to include them within your ipython notebook.\n",
    "\n",
    "- Your write up must be submitted in PDF form. You may use either Latex,  markdown, or any word processing software. <font color = 'darkred'>We will **NOT** accept handwritten work. </font> Make sure that your work is formatted correctly, for example submit $\\sum_{i=0} x_i$ instead of \\text{sum\\_\\{i=0\\} x\\_i}\n",
    "- When submitting the non-programming part of your assignment, you must correctly map pages of your PDF to each question/subquestion to reflect where they appear. Improperly mapped questions may not be graded correctly.\n",
    "- Discussion is encouraged on Edstem as part of the Q/A. You may discuss high-level ideas with other students at the \"whiteboard\" level (e.g. how cross validation works, using matmul instead of dot) and review any relevant materials online. However, all assignments should be done individually, each student must write up and submit their own answers.\n",
    "- **Graduate Students**: You are required to complete any sections marked as Bonus for Undergrads  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the autograder \n",
    "\n",
    "- You will find two assignments on Gradescope that correspond to HW2: \"Assignment 2 Programming\" and \"Assignment 2 Programming - Bonus for all\". \n",
    "<!-- No changes needed on the below section -->\n",
    "- You will submit your code for the autograder in the Assignment 2 Programming sections. Please refer to the Deliverables and Point Distribution section for what parts are considered required, bonus for undergrads, and bonus for all.\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n",
    "<!-- No changes needed on the above section -->\n",
    "\n",
    "- **For the \"Assignment 2 - Non-programming\" part, you will download your Jupyter Notebook as html and submit it as a PDF on Gradescope. To download the notebook as html, click on \"File\" on the top left corner of this page and select \"Download as > html\". Then, open the html file and print to PDF.**  Please refer to the Deliverables and Point Distribution section for an outline of the non-programming questions.\n",
    "- **When submitting to Gradescope, please make sure to mark the page(s) corresponding to each problem/sub-problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverables and Points Distribution \n",
    "\n",
    "### Q1: KMeans Clustering & DBScan [65 pts]\n",
    "#### Deliverables: <font color = 'green'>kmeans.py, dbscan.py, and Written Report</font>\n",
    "\n",
    "- **pairwise_dist** [5 pts] - *programming*\n",
    "\n",
    "- **KMeans Implementation** [35pts] - _programming_\n",
    "\n",
    "    - _init_centers [5pts]\n",
    "\n",
    "    - _update_assignment [10pts]\n",
    "\n",
    "    - _update_centers [10pts]\n",
    "\n",
    "    - _get_loss function [5pts]\n",
    "    \n",
    "    - _test_centers (Additional Autograder Tests - no need to implement) [5pts]\n",
    "- **Elbow Method** [15 pts]\n",
    "\n",
    "    - find_num_optimal_clusters [10pts] - *programming*\n",
    "\n",
    "    - written questions [5pts] - _non-programming_\n",
    "\n",
    "- **DBScan** [10 pts] - *programming*\n",
    "\n",
    "    - regionQuery [2pts]\n",
    "\n",
    "    - expandClusters [4pts]\n",
    "\n",
    "    - fit [4pts]\n",
    "\n",
    "### Q2: EM Algorithm [10pts; 20pts Bonus for Undergrad]\n",
    "#### Deliverables: <font color = 'green'>Written Report</font>\n",
    "\n",
    "- **2.1 Performing EM Update** [10 pts] - *non-programming*\n",
    "    - 2.1.1 [2pts] - *non-programming*\n",
    "\n",
    "    - 2.1.2 [2pts] - *non-programming*\n",
    "\n",
    "    - 2.1.3 [6pts] - *non-programming*\n",
    "- **2.2 EM Algorithm in Coin Toss Problem** [20 pts Undergrad Bonus] - *non-programming*\n",
    "\n",
    "    - 2.2.1 [10pts] - *non-programming*\n",
    "\n",
    "    - 2.2.2 [10pts] - *non-programming*\n",
    "\n",
    "### Q3: GMM implementation [55pts; 5pts Bonus for All] \n",
    "#### Deliverables: <font color = 'green'>gmm.py and Written Report</font>\n",
    "\n",
    "- 3.1 Helper Functions [15pts] - *programming & non-programming*\n",
    "\n",
    "    - 3.1.1. softmax [5pts]\n",
    "\n",
    "    - 3.1.2. logsumexp [3pts + 2pts] - *programming & non-programming*\n",
    "\n",
    "    - 3.1.3. normalPDF [5pts] - *for CS4641 students only*\n",
    "\n",
    "    - 3.1.3. multinormalPDF [5pts] - *for CS7641 students only*\n",
    "\n",
    "- 3.2 GMM Implementation [30pts] - *programming*\n",
    "\n",
    "    - 3.2.1. init_components [5pts]\n",
    "\n",
    "    - 3.2.2. _ll_joint [10pts]\n",
    "\n",
    "    - 3.2.3. Setup iterative steps for EM algorithm [15pts]\n",
    "    \n",
    "- 3.3 Image Compression and Pixel clustering [10pts] - *non-programming*\n",
    "- 3.4 Compare Full Convariance Matrix with Diagonal Covariance Matrix [5pts Bonus for All]\n",
    "\n",
    "### Q4: Cleaning Super Duper Messy data with semi-supervised learning [30pts Bonus for All] \n",
    "#### Deliverables: <font color = 'green'>semisupervised.py and Written Report</font>\n",
    "\n",
    "- 4.1: KNN [10pts] - *programming*\n",
    "\n",
    "- 4.2: Getting acquainted with semi-supervised learning approaches [5pts] - *non-programming*\n",
    "\n",
    "- 4.3: Implementing the EM algorithm [10pts] - *programming*\n",
    "\n",
    "- 4.4: *Demonstrating the performance of the algorithm* [5pts] - *programming*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Set up\n",
    "This notebook is tested under [python 3.\\*.\\*](https://www.python.org/downloads/release/python-368/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [matplotlib](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "\n",
    "Please implement the functions that have \"raise NotImplementedError\", and after you finish the coding, please delete or comment \"raise NotImplementedError\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version information\n",
      "python: 3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]\n",
      "matplotlib: 3.3.2\n",
      "numpy: 1.19.2\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import sys\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('matplotlib: {}'.format(matplotlib.__version__))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "\n",
    "# Set random seed so output is all same\n",
    "np.random.seed(1)\n",
    "\n",
    "# Load image\n",
    "import imageio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rob's Balloon Commission - KMeans Clustering [55 pts + 10 pts]\n",
    "\n",
    "Rob Boss, after being stuck in his dorm for multiple nights finishing Homework 1 for his Machine Learning class, decides to get some well deserved fresh air. He plans on visiting the hot air balloon festival downtown this weekend with a few friends. Once there, he takes many pictures and ultimately decides that his favorite is the picture attached below (please compliment Rob on his photography and machine learning skills the next time you see him) and wants to hang it up in his dorm. He decides to commission his artist friend to convert his picture into a painting but finds out his friend charges him for each color used when painting. Being the cash-strapped college student he is, help Rob build a KMeans clustering algorithm that can help him decide how many colors he wants to commission for his painting so that he can see the pattern of the hot air balloons without spending too much money."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='balloon.png' width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans is trying to solve the following optimization problem:\n",
    "\n",
    "\\begin{align}\n",
    "\\arg \\min_S \\sum_{i=1}^K \\sum_{x_j \\in S_i} ||x_j - \\mu_i||^2\n",
    "\\end{align}\n",
    "where one needs to partition the N observations into K clusters: $S = \\{S_1, S_2, \\ldots, S_K\\}$ and each cluster has $\\mu_i$ as its center.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 pairwise distance [5pts]\n",
    "\n",
    "In this section, you are asked to implement pairwise_dist function.\n",
    "\n",
    "Given $X \\in \\mathbb{R}^{N x D}$ and $Y \\in \\mathbb{R}^{M x D}$, obtain the pairwise distance matrix $dist \\in \\mathbb{R}^{N x M}$ using the euclidean distance metric, where $dist_{i, j} = ||X_i - Y_j||_2$.  \n",
    "\n",
    "DO NOT USE FOR LOOPs in your implementation -- they are slow and will make your code too slow to pass our grader. [Use array broadcasting instead](https://numpy.org/doc/stable/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kmeans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bb64931e4367>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m###############################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_dist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kmeans'"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from kmeans import pairwise_dist\n",
    "\n",
    "x = np.random.randn(2, 2)\n",
    "y = np.random.randn(3, 2)\n",
    "\n",
    "print(\"*** Expected Answer ***\")\n",
    "print(\"\"\"==x==\n",
    "[[ 1.62434536 -0.61175641]\n",
    " [-0.52817175 -1.07296862]]\n",
    "==y==\n",
    "[[ 0.86540763 -2.3015387 ]\n",
    " [ 1.74481176 -0.7612069 ]\n",
    " [ 0.3190391  -0.24937038]]\n",
    "==dist==\n",
    "[[1.85239052 0.19195729 1.35467638]\n",
    " [1.85780729 2.29426447 1.18155842]]\"\"\")\n",
    "\n",
    "\n",
    "print(\"\\n*** My Answer ***\")\n",
    "print(\"==x==\")\n",
    "print(x)\n",
    "print(\"==y==\")\n",
    "print(y)\n",
    "print(\"==dist==\")\n",
    "print(pairwise_dist(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 KMeans Implementation [30pts]\n",
    "\n",
    "In this section, you are asked to implement _init_centers [5pts], _update_assignment [10pts], _update_centers [10pts] and _get_loss function [5pts].\n",
    "\n",
    "For the function signature, please see the corresponding doc strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "from kmeans import KMeans\n",
    "\n",
    "def image_to_matrix(image_file, grays=False):\n",
    "    \"\"\"\n",
    "    Convert .png image to matrix\n",
    "    of values.\n",
    "    params:\n",
    "    image_file = str\n",
    "    grays = Boolean\n",
    "    returns:\n",
    "    img = (color) np.ndarray[np.ndarray[np.ndarray[float]]]\n",
    "    or (grayscale) np.ndarray[np.ndarray[float]]\n",
    "    \"\"\"\n",
    "    img = plt.imread(image_file)\n",
    "    # in case of transparency values\n",
    "    if len(img.shape) == 3 and img.shape[2] > 3:\n",
    "        height, width, depth = img.shape\n",
    "        new_img = np.zeros([height, width, 3])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r, c, :] = img[r, c, 0:3]\n",
    "        img = np.copy(new_img)\n",
    "    if grays and len(img.shape) == 3:\n",
    "        height, width = img.shape[0:2]\n",
    "        new_img = np.zeros([height, width])\n",
    "        for r in range(height):\n",
    "            for c in range(width):\n",
    "                new_img[r, c] = img[r, c, 0]\n",
    "        img = new_img\n",
    "    return img\n",
    "\n",
    "def update_image_values(k):\n",
    "    cluster_idx, centers, loss = KMeans()(image_values, k)\n",
    "    updated_image_values = np.copy(image_values)\n",
    "\n",
    "    # assign each pixel to cluster mean\n",
    "    for i in range(0,k):\n",
    "        indices_current_cluster = np.where(cluster_idx == i)[0]\n",
    "        updated_image_values[indices_current_cluster] = centers[i]\n",
    "\n",
    "    updated_image_values = updated_image_values.reshape(r,c,ch)\n",
    "    return updated_image_values\n",
    "\n",
    "def plot_image(img_list, title_list, figsize=(9, 12)):\n",
    "    fig, axes = plt.subplots(1, len(img_list), figsize=figsize)\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(img_list[i])\n",
    "        ax.set_title(title_list[i])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "#Note that because of a different file structure, students' paths will be different\n",
    "\n",
    "image_values = image_to_matrix('./images/balloon.png')\n",
    "\n",
    "r = image_values.shape[0]\n",
    "c = image_values.shape[1]\n",
    "ch = image_values.shape[2]\n",
    "# flatten the image_values\n",
    "image_values = image_values.reshape(r*c,ch)\n",
    "\n",
    "print('Loading...')\n",
    "\n",
    "image_2 = update_image_values(2).reshape(r, c, ch)\n",
    "image_5 = update_image_values(5).reshape(r, c, ch)\n",
    "image_10 = update_image_values(10).reshape(r, c, ch)\n",
    "image_20 = update_image_values(20).reshape(r, c, ch)\n",
    "\n",
    "plot_image([image_2, image_5, image_10, image_20], ['K = 2', 'K = 5', 'K = 10', 'K = 20'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Elbow Method [6 pts Programming + 4 pts Written Questions]\n",
    "\n",
    "One of the biggest drawbacks of KMeans is that we need to know the number of clusters beforehand. Let's see if we can help Rob's paint optimization problem by implementing the elbow method to find the optimal number of clusters in the function find_optimal_num_clusters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "from kmeans import find_optimal_num_clusters\n",
    "\n",
    "find_optimal_num_clusters(image_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Written Questions:\n",
    "\n",
    "1) Approximately what value does the elbow method give? Was the elbow method roughly accurate when compared to the images displayed? \n",
    " \n",
    "2) Rob is not quite sure to interpret his loss graph using the elbow method, so he decides to optimize the number of clusters by choosing k to have the loss be as close to 0 as possible. Would this idea be a good solution for making sure he gets the most bang for his buck when it comes to displaying balloon patterns? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions:\n",
    "\n",
    "1) \n",
    "\n",
    "2) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitation of K-Means\n",
    "\n",
    "One of the limitations of K-Means Clustering is that it dependes largely on the shape of the dataset. A common example of this is trying to cluster one circle within another (concentric circles). A K-means classifier will fail to do this and will end up effectively drawing a line which crosses the circles. You can visualize this limitation in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "# visualize limitation of kmeans\n",
    "from sklearn.datasets import (make_circles, make_moons)\n",
    "\n",
    "X1, y1 = make_circles(factor=0.5, noise=0.05, n_samples=1500)\n",
    "X2, y2 = make_moons(noise=0.05, n_samples=1500)\n",
    "\n",
    "def visualise(X, C, K=None):# Visualization of clustering. You don't need to change this function   \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=C,cmap='rainbow')\n",
    "    if K:\n",
    "        plt.title('Visualization of K = '+str(K), fontsize=15)\n",
    "    plt.show()\n",
    "    pass\n",
    "\n",
    "cluster_idx1, centers1, loss1 = KMeans()(X1, 2)\n",
    "visualise(X1, cluster_idx1, 2)\n",
    "\n",
    "cluster_idx2, centers2, loss2 = KMeans()(X2, 2)\n",
    "visualise(X2, cluster_idx2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Autograder test to find centers for data points [5 pts]\n",
    "\n",
    "To obtain these 5 points, you need to be pass the tests set up in the autograder. These will test the centers created by your implementation. Be sure to upload the correct files to obtain these points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 DBSCAN [10 pts]\n",
    "\n",
    "Let us try to solve these limitations using another clustering algorithm: DBSCAN. As mentioned in lecture, DBSCAN tries to find dense regions in the data spce, separated by regions of lower density. DBSCAN is parameterized by two parameters (eps and minPts):\n",
    "* $\\epsilon$: Maximum radius of neighborhood\n",
    "* $MinPts$: Maximum number of points in Eps-neighborhood of a point\n",
    "\n",
    "The algorithm fits on a dataset as follows:\n",
    "<br/> <br/>\n",
    "For each unvisited point in dataset:\n",
    "1. Set point as visited \n",
    "2. Extract the Eps-neighborhood $Neighbors$ of this point using regionQuery() (all points which are within $\\epsilon$ distance of this neighborhood)\n",
    "3. If the size of this neighbohood is greater than $minPts$, then we run expandCluster(), which does the following for cluster $C$:\n",
    "    1. Set point $P$ as visited\n",
    "    2. Add $P$ to cluster $C$\n",
    "    3. For each point $P'$ in $P$'s neighborhood $Neighbors$:\n",
    "        1. Set point $P'$ as visited \n",
    "        2. Extract the Eps-neighborhood $Neighbors'$ of this point using regionQuery()\n",
    "        3. If the size of $Neighbors'$ is greater than minPts, we add $Neighbors'$ to our current neighborhood $Neighbors$: \n",
    "        4. If $P'$ has not been assigned a cluster yet, we add it to cluster $C$\n",
    "    4. Move onto next cluster $C + 1$\n",
    "        \n",
    "Using the above description, complete fit(), expandCluster(), and regionQuery() in dbscan.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, test your code by running the cell below. You should be able to get a perfect clustering for the two circles dataset, which you can observe quantitatively by checking whether the clusters returned by cluster_idx and the ground truth clusters are the same and qualitatively by visualizing the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "BEST_EPS = 0.11\n",
    "BEST_POINTS = 3\n",
    "from dbscan import DBSCAN\n",
    "dbscan = DBSCAN(BEST_EPS, BEST_POINTS, X1)\n",
    "cluster_idx = dbscan.fit()\n",
    "## Note that one of the two cells should print True for a correct implementation\n",
    "print(np.array_equal(y1, cluster_idx)) #Checks if y1 == cluster_idx\n",
    "print(np.array_equal(y1, 1-cluster_idx)) ## Checks if y1 is the exact opposite of cluster_idx (1s instead of 0s and vice-versa)\n",
    "visualise(X1, cluster_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EM algorithm [30 pts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Performing EM Update [10 pts]\n",
    "\n",
    "A univariate Gaussian Mixture Model (GMM) has two components, both of which have their own mean and standard deviation. The model is defined by the following parameters:\n",
    "\n",
    "$$ \\mathbf{z} \\sim Bernoulli(\\theta) $$\n",
    "$$ \\mathbf{p(x|z=0)} \\sim \\mathcal{N}(2\\mu, 5\\sigma^{2}) $$\n",
    "$$ \\mathbf{p(x|z=1)} \\sim \\mathcal{N}(\\mu, \\sigma^{2}) $$\n",
    "\n",
    "For a dataset of N datapoints, find the following: \n",
    "\n",
    "\n",
    "2.1.1. Write the marginal probability of x, i.e. $\\mathbf{p(x)}$  \\[2pts] \n",
    "\n",
    "\n",
    "2.1.2. E-Step: Compute the posterior probability, i.e, $p(z^i=k|x^i)$, where k = {0,1} \\[2pts]\n",
    "\n",
    "\n",
    "2.1.3. M-Step: Compute the updated value of $\\sigma^{2}$ (You can keep $\\mu$ fixed for this) \\[6pts]\n",
    "\n",
    "#### Answers:\n",
    "\n",
    "2.1.1\n",
    "\n",
    "\n",
    "2.1.2\n",
    "\n",
    "\n",
    "2.1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 EM Algorithm in Coin Toss Problem [20 pts Undergrad Bonus]\n",
    "\n",
    "Suppose we have 2 coins, ${C_1}$ and ${C_2}$. We have the following table that shows information related to 6 trials along with their head and tail results. In each trial, one of the coins is flipped exactly 8 times, meaning every row of the table represents a single trial performed by a single coin. For example, let's say we knew that the first trial with results TTHTHHHT was performed by ${C_1}$. Then we know that for that trial, ${C_1}$ obtained 4 heads in total and ${C_2}$ obtained 0 heads in total since it was not the one flipped on this specific trial.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\\hline Coin & Flip Results & Coin 1 Heads & Coin 2 Heads \n",
    "\\\\ \\hline & TTHTHHHT   \n",
    "\\\\ \\hline & HHTTHHHT \n",
    "\\\\ \\hline & TTTTTHHT  \n",
    "\\\\ \\hline & THTHTTHT  \n",
    "\\\\ \\hline & HHTHHTHH \n",
    "\\\\ \\hline & THTTTTTT\n",
    "\\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "As you can see, we know the results of the coin flips for each trial, but we don't know which coin was flipped on each trial. (Example: we know that the first trial resulted in TTHTHHHT, but we don't know which coin out of the two coins available was used to produce such results). Therefore, we also can't count how many heads a coin produced in a trial since we don't know which coin was flipped on such trial.\n",
    "\n",
    "Using the information we have available, we want to take advantage of the EM Algorithm to figure out the following:\n",
    "- Make an initial guess of the biases of the 2 coins to estimate the order of coins for the 6 trials performed\n",
    "- Estimate the probability that each coin could have been thrown at a certain trial\n",
    "- Recalculate to make our biases guess a better one in the end of the EM process\n",
    "\n",
    "To start, it's important to note that both coins have equal chance of being picked for each trial:\n",
    "- ${P(C_1) = 0.5}$\n",
    "- ${P(C_2) = 0.5}$\n",
    "\n",
    "Regarding the biases of the coins, we will start by assuming that they have the following:\n",
    "- ${C_1 = 0.4}$ heads (and ${0.6}$ tails)\n",
    "- ${C_2 = 0.7}$ heads (and ${0.3}$ tails)\n",
    "\n",
    "2.2.1 You will fill in the table below, calculating the probability that ${C_x}$ was picked for each trial, as well as the amount of heads flipped by ${C_x}$ for each trial. (You can round your answers to 3 decimal places). Some entries in the table are already filled as hints to your calculations. To be specific, each column of the table represents: [10pts]\n",
    "\n",
    "- P(C1|T) is the probability that coin 1 (${C_1}$) was picked for the trial\n",
    "- P(C2|T) is the probability that coin 2 (${C_2}$) was picked for the trial\n",
    "- Coin 1 Heads is the amount of heads flipped in a certain trial times the probability that Coin 1 was picked for the trial\n",
    "- Coin 2 Heads is the amount of heads flipped in a certain trial times the probability that Coin 2 was picked for the trial\n",
    "\n",
    "(Hint: you will find Bayes' Theorem useful for this question ${P(A|B)=\\frac{P(B|A)*P(A)}{P(B)}=\\frac{P(B|A)*P(A)}{P(B|A)*P(A)+P(B|-A)*P(-A)}}$)\n",
    "\n",
    "(Hint: ${P(T_1|C_1) = P(TTHTHHHT|\\text{picked} C_1) = \\frac{8!}{4! * 4!} * 0.4^{4} * 0.6^{4}}$)\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\\hline Flip Results & P(C1|T) & P(C2|T) & Coin 1 Heads & Coin 2 Heads\n",
    "\\\\ \\hline TTHTHHHT & 0.630 &&&\n",
    "\\\\ \\hline HHTTHHHT &&&& 3.360\n",
    "\\\\ \\hline TTTTTHHT & &&1.908&\n",
    "\\\\ \\hline THTHTTHT & &0.878\n",
    "\\\\ \\hline HHTHHTHH &\n",
    "\\\\ \\hline THTTTTTT &\n",
    "\\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "2.2.2 Now you want to apply the M-Step of the algorithm to improve our first estimate on the coin biases, calculating\n",
    "${\\theta^{new_1}}$ and ${\\theta^{new_2}}$. You would be maximizing the likelihood of the estimated count of flips we previously calculated in the E-Step [10pts]\n",
    "\n",
    "(Hint: $\\theta_{new_x} = \\frac{\\sum c_{x}\\text{heads}}{\\text{number of flips in a trial} * \\sum_{i=1}^{n}P(C_x|T_i)}$, where $n = $ number of trials)\n",
    "#### Answer:\n",
    "\n",
    "2.2.1\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\\hline Flip Results & P(C1|T) & P(C2|T) & Coin 1 Heads & Coin 2 Heads \n",
    "\\\\ \\hline TTHTHHHT &  &  &  &   \n",
    "\\\\ \\hline HHTTHHHT &  &  &  & \n",
    "\\\\ \\hline TTTTTHHT &  &  &  & \n",
    "\\\\ \\hline THTHTTHT &  &  &  & \n",
    "\\\\ \\hline HHTHHTHH &  &  &  & \n",
    "\\\\ \\hline THTTTTTT &  &  &  & \n",
    "\\\\\\hline\\end{array}\n",
    "$$\n",
    "\n",
    "2.2.2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GMM implementation [43pts Programming + 12pts Written Questions + 5pts Bonus for all pts]\n",
    "\n",
    "A Gaussian Mixture Model(GMM) is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian Distribution. In a nutshell, GMM is a soft clustering algorithm in a sense that each data point is assigned to a cluster with a probability. In order to do that, we need to convert our clustering problem into an inference problem.\n",
    "\n",
    "Given $N$ samples $X = [x_1, x_2, \\ldots, x_N]^T$, where $x_i \\in \\mathbb{R}^D$. Let $\\pi$ be a K-dimensional probability distribution and $(\\mu_k; \\Sigma_k)$ be the mean and covariance matrix of the $k^{th}$ Gaussian distribution in $\\mathbb{R}^d$. \n",
    "\n",
    "The GMM object implements EM algorithms for fitting the model and MLE for optimizing its parameters. It also has some particular hypothesis on how the data was generated:\n",
    "\n",
    "- Each data point $x_i$ is assigned to a cluster $k$ with probability of $\\pi_k$ where $\\sum_{k=1}^K \\pi_k = 1$\n",
    "- Each data point $x_i$ is generated from Multivariate Normal Distribution $\\cal{N}(\\mu_k, \\Sigma_k)$ where $\\mu_k \\in \\mathbb{R}^D$ and $\\Sigma_k \\in \\mathbb{R}^{D\\times D}$\n",
    "\n",
    "Our goal is to find a $K$-dimension Gaussian distributions to model our data $X$. This can be done by learning the parameters $\\pi, \\mu$ and $\\Sigma$ through likelihood function. Detailed derivation can be found in our slide of GMM. The log-likelihood function now becomes:\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{ln } p(x_1, \\dots, x_N | \\pi, \\mu, \\Sigma) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi(k) \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)\\big)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture we know that MLEs for GMM all depend on each other and the responsibility $\\tau$. Thus, we need to use an iterative algorithm (the EM algorithm) to find the estimate of parameters that maximize our likelihood function. **All detailed derivations can be found in the lecture slide of GMM.**\n",
    "\n",
    "- **E-step:** Evaluate the responsibilities\n",
    "\n",
    "In this step, we need to calculate the responsibility $\\tau$, which is the conditional probability that a data point belongs to a specific cluster $k$ if we are given the datapoint, i.e. $P(z_k|x)$. The formula for $\\tau$ is given below:\n",
    "\n",
    "$$\n",
    "\\tau\\left(z_k\\right)=\\frac{\\pi_{k} N\\left(x | \\mu_{k}, \\Sigma_{k}\\right)}{\\sum_{j=1}^{K} \\pi_{j} N\\left(x | \\mu_{j}, \\Sigma_{j}\\right)}, \\quad \\text{for } k = 1, \\dots, K\n",
    "$$\n",
    "Note that each data point should have one probability for each component/cluster. For this homework, you will work with $\\tau\\left(z_k\\right)$ which has a size of $N\\times K$ and you should have all the responsibility values in one matrix. **We use gamma as $\\tau$ in this homework**.\n",
    "\n",
    "- **M-step:** Re-estimate Paramaters\n",
    "\n",
    "After we obtained the responsibility, we can find the update of parameters, which are given below:\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_k^{new} &= \\dfrac{\\sum_{n=1}^N \\tau(z_k)x_n}{N_k} \\\\\n",
    "\\Sigma_k^{new} &= \\dfrac{1}{N_k}\\sum_{n=1}^N \\tau (z_k)^T(x_n - \\mu_k^{new})^T(x_n-\\mu_k^{new}) \\\\\n",
    "\\pi_k^{new} &= \\dfrac{N_k}{N}\n",
    "\\end{align}\n",
    "where $N_k = \\sum_{n=1}^N \\tau(z_k)$. Note that the updated value for $\\mu_k$ is used when updating $\\Sigma_k$. The multiplication of $\\tau (z_k)^T(x_n - \\mu_k^{new})^T$ is element-wise so it will preserve the dimensions of $(x_n - \\mu_k^{new})^T$.\n",
    "\n",
    "- We repeat E and M steps until the incremental improvement to the likelihood function is small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special Notes**\n",
    "- For undergaduate student: you may assume that the covariance matrix $\\Sigma$ is diagonal matrix, which means the features are independent. (i.e. the red intensity of a pixel is independent from its blue intensity, etc). Make sure you set **Full_matrix = False** before you submit your code to Gradscope. \n",
    "- For graduate student: please assume full covariance matrix. Make sure you set **Full_matrix = True** before you submit your code to Gradscope\n",
    "- The class notes assume that your dataset $X$ is $(D, N)$ but **the homework dataset is $(N, D)$ as mentioned on the instructions, so the formula is a little different from the lecture note in order to obtain the right dimensions of parameters.**\n",
    "\n",
    "**Hints**\n",
    "\n",
    "1. **DO NOT USE FOR LOOPS OVER N.** You can always find a way to avoid looping over the observation datapoints in our homework problem. If you have to loop over D or K, that would be fine.\n",
    "\n",
    "2. You can initiate $\\pi(k)$ the same for each $k$, i.e. $\\pi(k) = \\frac{1}{K}, \\forall k = 1, 2, \\ldots, K$.\n",
    "\n",
    "3. In part 3 you are asked to generate the model for pixel clustering of image. We will need to use a multivariate Gaussian because each image will hava $N$ pixels and $D=3$ features which corresponds to red, green, and blue color intensities. It means that each image is a $(N\\times3)$ dataset matrix. In the following parts, remember $D=3$ in this problem.\n",
    "\n",
    "4. To avoid using for loops in your code, we recommend you take a look at the concept [Array Broadcasting in Numpy](https://numpy.org/doc/stable/user/theory.broadcasting.html#array-broadcasting-in-numpy). Also, certain calculations that required different shapes of arrays can also be achieved by broadcasting. \n",
    "\n",
    "5. Be careful of the dimensions of your parameters. Before you test anything on the autograder, please look at the instructions below on the shapes of the variables you need to output and how to format your return statement. Print the shape of an array by [print(array.shape)](https://www.w3schools.com/python/numpy/numpy_array_shape.asp) could enhance the functionality of your code and help you debugging. Also notice that **a numpy array in shape $(N,1)$ is NOT the same as that in shape $(N,)$** so be careful and consistent on what you are using. You can see the detailed explanation here. [Difference between numpy.array shape (R, 1) and (R,)](https://stackoverflow.com/questions/22053050/difference-between-numpy-array-shape-r-1-and-r)\n",
    " - The dataset $X$: $(N, D)$\n",
    " - $\\mu$: $(K, D)$. \n",
    " - $\\Sigma$: $(K, D, D)$\n",
    " - $\\tau$: $(N, K)$\n",
    " - $\\pi$: array of length $K$\n",
    " - ll_joint: $(N, K)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Helper functions [15 pts]\n",
    "\n",
    "To facilitate some of the operations in the GMM implementation, we would like you to implement the following three helper functions. In these functions, \"logit\" refers to an input array of size $(N, D)$. Remember the goal of helper functions is to facilitate our calculation so **DO NOT USE FOR LOOP ON N**.\n",
    "\n",
    "### 3.1.1. softmax [5 pts]\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N \\times D}$, calculate $prob \\in \\mathbb{R}^{N \\times D}$, where $prob_{i, j} = \\frac{\\exp(logit_{i, j})}{\\sum_{d=1}^D exp(logit_{i, d})}$.\n",
    "\n",
    "Note: it is possible that $logit_{i, j}$ is very large, making $\\exp(\\cdot)$ of it to explode. To make sure it is numerically stable, you need to subtract the maximum for each row of $logits$.\n",
    "\n",
    "\n",
    "**Special Notes**\n",
    "- Do not add back the maximum for each row. \n",
    "- Add **keepdims=True** in your np.sum() function to avoid broadcast error. \n",
    "\n",
    "### 3.1.2. logsumexp [3 pts Programming + 2 pts Written Question]\n",
    "\n",
    "Given $logit \\in \\mathbb{R}^{N \\times D}$, calculate $s \\in \\mathbb{R}^N$, where $s_i = \\log \\big( \\sum_{j=1}^D \\exp(logit_{i, j}) \\big)$. Again, pay attention to the numerical problem. You may face similar condition as in the softmax function. In this case, add the maximum for each row of $logit$ back for your functions Note: This function is used in the call() function which is given, so you will not need it in your own implementation. It helps calculate the loss of log-likehood. \n",
    "\n",
    "#### Written Questions [2 pts]:\n",
    "\n",
    "1) Why **logsumexp()** function add the maximum for each row of $logit$ back?\n",
    "\n",
    "**Hint**: start with a simple example like $logit \\in \\mathbb{R}^{1 \\times D}$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.3. Multivariate Gaussian PDF [5 pts]\n",
    "You should be able to write your own function based on the following formula, and you are **NOT allowed** to use outside resource packages other than those we provided. \n",
    "\n",
    "**(for undergrads only) normalPDF**\n",
    "\n",
    "Using the covariance matrix as a diagonal matrix with variances of the individual variables appearing on the main diagonal of the matrix and zeros everywhere else means that we assume the features are independent. In this case, the multivariate normal density function simplifies to the expression below:\n",
    "$$\\mathcal{N}(x: \\mu, \\Sigma) = \\prod_{i=1}^D \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}\\exp{\\left( -\\frac{1}{2\\sigma_i^2} (x_i-\\mu_i)^2\\right)}$$\n",
    "where $\\sigma^2_i$ is the variance for the $i^{th}$ feature, which is the diagonal element of the covariance matrix.\n",
    "\n",
    "**(for grads only) multinormalPDF**\n",
    "\n",
    "Given the dataset $X \\in \\mathbb{R}^{N \\times D}$, the mean vector $\\mu \\in \\mathbb{R}^{D}$ and covariance matrix $\\Sigma \\in \\mathbb{R}^{D \\times D}$ for a multivariate Gaussian distrubution, calculate the probability $p \\in \\mathbb{R}^{N}$ of each data. The PDF is given by \n",
    "$$\\mathcal{N}(X: \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{D/2}}|\\Sigma|^{-1/2}\\exp{\\left(-\\frac{1}{2}(X-\\mu)\\Sigma^{-1}(X-\\mu)^T\\right)}$$\n",
    "where $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "\n",
    "\n",
    "**Hints**\n",
    "- If you encounter \"LinAlgError\", you can mitigate your number/array by summing a small value before taking the operation, e.g. np.linalg.inv($\\Sigma_k$ + SIGMA_CONST). You can arrest and handle such error by using [Try and Exception Block](https://realpython.com/python-exceptions/#the-try-and-except-block-handling-exceptions) in Python.\n",
    "\n",
    "- In the above calculation, you must avoid computing a $(N,N)$ matrix. Using the above equation for large N will crash your kernel and/or give you a memory error on Gradescope. Instead, you can do this same operation by calculating $(X-\\mu)\\Sigma^{-1}$, a $(N,D)$ matrix, transpose it to be a $(D,N)$ matrix and do an element-wise multiplication with $(X-\\mu)^T$, which is also a $(D,N)$ matrix. Lastly, you will need to sum over the 0 axis to get a $(1,N)$ matrix before proceeding with the rest of the calculation. This uses the fact that doing an element-wise multiplication and summing over the 0 axis is the same as taking the diagonal of the $(N,N)$ matrix from the matrix multiplication. \n",
    "- In Numpy implementation for each individual $\\mu$, you can either use a 2-D array with dimension $(1,D)$ for each Gaussian Distribution, or a 1-D array with length $D$. Same to other array parameters. Both ways should be acceptable but pay attention to the shape mismatch problem and be **consistent all the time** when you implement such arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GMM Implementation [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do in this problem:\n",
    "### 3.2.1. Initialize parameters in _init_components() [5 pts]\n",
    "\n",
    "Examples of how you can initialize the parameters. \n",
    "  1. Set the prior probability $\\pi$ the same for each class.\n",
    "  2. Initialize $\\mu$ by randomly selecting K numbers of observations as the initial mean vectors. You can use [int(np.random.uniform())](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html) to get the row index number of the datapoints randomly. \n",
    "  3. Initialize the covariance matrix with [np.eye()](https://numpy.org/devdocs/reference/generated/numpy.eye.html) for each k. For grads, you can also initialize the $\\Sigma$ by K diagonal matrices. It will become a full matrix after one iteration, as long as you adopt the correct computation.\n",
    "  4. Other ways of initialization are acceptable and welcome. The autograder will only test the shape of your $\\pi$,$\\mu$, $\\sigma$. Make sure you pass other evaluations in the autograder.\n",
    "\n",
    "### 3.2.2. Formulate the log-likelihood function _ll_joint() [10 pts]\n",
    "\n",
    "The log-likelihood function is given by:\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ell(\\theta) = \\sum_{i=1}^N \\text{ln } \\big( \\sum_{k=1}^{K} \\pi(k) \\mathcal{N}(x_i | \\mu_k, \\Sigma_k)\\big)\n",
    "\\end{align}\n",
    "$$\n",
    "In this part, we will generate a $(N,K)$ matrix where each datapoint $x_i, \\forall i = 1, \\dots, N$ has $K$ log-likelihood numbers. Thus, for each $i = 1, \\dots, N$ and $k = 1, \\dots, K$, \n",
    "$$\n",
    "\\text{log-likelihood}[i,k] = \\log{\\pi_k}+\\log{\\cal{N}(x_i|\\mu_k, \\Sigma_k)}\n",
    "$$\n",
    "\n",
    "**Hints:**\n",
    "- If you encounter \"ZeroDivisionError\" or \"RuntimeWarning: divide by zero encountered in log\", you can mitigate your number/array by summing a small value before taking the operation, e.g. np.log($\\pi_k$ + 1e-32). \n",
    "- You need to use the Multivariate Normal PDF function you created in the last part. Remember the PDF function is for each Gaussian Distribution (i.e. for each k) so you need to use a for loop over K. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3. Setup Iterative steps for EM Algorithm  [5+10 pts]\n",
    "\n",
    "You can find the detail instruction in the above description box. \n",
    "\n",
    "**Hints:**\n",
    "- For E steps, we already get the log-likelihood at _ll_joint() function. This is not the same as responsibilities ($\\tau$), but you should be able to finish this part with just a few lines of code by using _ll_joint() and softmax() defined above. \n",
    "- For undergrads: Try to simplify your calculation for $\\Sigma$ in M steps as you assumed independent components. Make sure you are only taking the diagonal terms of your calculated covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Tests###\n",
    "\n",
    "Use these to test if your implementation of functions in GMM work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from gmm import GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "data = np.random.randn(4, 3)\n",
    "\n",
    "# test softmax utility\n",
    "my_softmax = GMM(data, 3).softmax(data)\n",
    "expected_softmax = np.array([[0.81761761, 0.08738232, 0.09500007],\n",
    "       [0.12135669, 0.84312089, 0.03552242],\n",
    "       [0.75647821, 0.0617229 , 0.18179889],\n",
    "       [0.14923883, 0.82635643, 0.02440474]])\n",
    "\n",
    "print(\"Your softmax works within the expected range: \", np.allclose(expected_softmax, my_softmax))\n",
    "\n",
    "\n",
    "# test logsumexp utility\n",
    "my_logsumexp = GMM(data, 3).logsumexp(data)\n",
    "expected_logsumexp = np.array([[1.82570589],\n",
    " [1.03605256],\n",
    " [2.02389331],\n",
    " [1.65283702]])\n",
    "\n",
    "print(\"Your logsumexp works within the expected range: \", np.allclose(expected_logsumexp, my_logsumexp))\n",
    "\n",
    "\n",
    "# init random\n",
    "points = np.random.randn(12, 3)\n",
    "\n",
    "mu = np.array([[-0.74715829,  1.6924546,   0.05080775],\n",
    " [-1.09989127, -0.17242821, -0.87785842],\n",
    " [-0.3224172,  -0.38405435,  1.13376944]])\n",
    "\n",
    "sigma = np.array([[[1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.]],\n",
    "       [[1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.]],\n",
    "       [[1., 0., 0.],\n",
    "        [0., 1., 0.],\n",
    "        [0., 0., 1.]]])\n",
    "pi = np.ones(3)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "# For undergrads\n",
    "\n",
    "# test normalPDF\n",
    "my_normalpdf = GMM(points, 3).normalPDF(points, mu[0], sigma[0])\n",
    "expected_normal_pdf = np.array([0.0037374 , 0.00681159, 0.01294674, 0.00700474, 0.00095577,\n",
    "       0.00813925, 0.00544499, 0.00385966, 0.00561288, 0.00228524,\n",
    "       0.06349364, 0.00250289])\n",
    "\n",
    "print(\"Your normal pdf works within the expected range: \", np.allclose(expected_normal_pdf, my_normalpdf))\n",
    "\n",
    "\n",
    "# test ll-joint\n",
    "my_lljoint = GMM(points, 3)._ll_joint(pi, mu, sigma,False)\n",
    "expected_lljoint = np.array([[-6.68797812, -6.20337699, -3.85542789],\n",
    "       [-6.08774253, -3.85542789, -6.20337698],\n",
    "       [-5.44552376, -4.81763731, -6.88557037],\n",
    "       [-6.05977986, -7.90402129, -5.95737486],\n",
    "       [-8.05161039, -6.27262476, -5.43812535],\n",
    "       [-5.90966969, -4.86498535, -4.23232988],\n",
    "       [-6.31167075, -3.98209541, -5.58159406],\n",
    "       [-6.65578947, -4.38655011, -4.69047683],\n",
    "       [-6.28130441, -7.15820124, -4.50096327],\n",
    "       [-7.17989628, -5.55202701, -6.48346667],\n",
    "       [-3.85542789, -6.08774255, -6.6879781 ],\n",
    "       [-7.08892294, -8.46315357, -4.53725014]])\n",
    "\n",
    "print(\"Your lljoint works within the expected range: \", np.allclose(my_lljoint, expected_lljoint))\n",
    "\n",
    "# test E step\n",
    "my_estep = GMM(points, 3)._E_step(pi, mu, sigma)\n",
    "expected_estep = np.array([[0.05098852, 0.08278125, 0.86623023],\n",
    "       [0.08918842, 0.83136246, 0.07944912],\n",
    "       [0.3214852 , 0.60234958, 0.07616522],\n",
    "       [0.44131069, 0.06979118, 0.48889813],\n",
    "       [0.04861361, 0.28797946, 0.66340693],\n",
    "       [0.10876892, 0.30917578, 0.5820553 ],\n",
    "       [0.074913  , 0.76962456, 0.15546244],\n",
    "       [0.0561508 , 0.54309286, 0.40075634],\n",
    "       [0.13609235, 0.05662422, 0.80728343],\n",
    "       [0.12346309, 0.62879889, 0.24773802],\n",
    "       [0.85752822, 0.09199548, 0.0504763 ],\n",
    "       [0.07101476, 0.01796916, 0.91101608]])\n",
    "\n",
    "\n",
    "print(\"Your E step works within the expected range: \", np.allclose(my_estep, expected_estep))\n",
    "\n",
    "# test M step\n",
    "my_pi, my_mu, my_sigma = GMM(points, 3)._M_step(expected_estep, False)\n",
    "expected_pi = np.array([0.19829313, 0.35762874, 0.44407813])\n",
    "expected_mu = np.array([[-0.20989007,  0.79579186,  0.06554929],\n",
    "        [-0.35741548, -0.1535599 , -0.4876455 ],\n",
    "        [-0.28772515, -0.07512445,  0.79292111]])\n",
    "expected_sigma = np.array([[[0.64857055, 0.        , 0.        ],\n",
    "         [0.        , 0.63446774, 0.        ],\n",
    "         [0.        , 0.        , 0.62167826]],\n",
    " \n",
    "        [[0.53473119, 0.        , 0.        ],\n",
    "         [0.        , 0.23538075, 0.        ],\n",
    "         [0.        , 0.        , 0.38671205]],\n",
    " \n",
    "        [[0.62612107, 0.        , 0.        ],\n",
    "         [0.        , 0.24611766, 0.        ],\n",
    "         [0.        , 0.        , 0.88668642]]])\n",
    "print(\"Your M step works within the expected range: \", np.allclose(my_pi, expected_pi) and np.allclose(my_mu, expected_mu) and np.allclose(my_sigma, expected_sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "# For grads\n",
    "\n",
    "# # test mutlinormalPDF\n",
    "sigma_grad = np.array([[[ 0.12015895,  0.61720311,  0.30017032],\n",
    "        [-0.35224985, -1.1425182 , -0.34934272],\n",
    "        [-0.20889423,  0.58662319,  0.83898341]],\n",
    "\n",
    "       [[ 0.93110208,  0.28558733,  0.88514116],\n",
    "        [-0.75439794,  1.25286816,  0.51292982],\n",
    "        [-0.29809284,  0.48851815, -0.07557171]],\n",
    "\n",
    "       [[ 1.13162939,  1.51981682,  2.18557541],\n",
    "        [-1.39649634, -1.44411381, -0.50446586],\n",
    "        [ 0.16003707,  0.87616892,  0.31563495]]])\n",
    "my_multinormalpdf = GMM(data, 3).multinormalPDF(points, mu[0], sigma_grad[0])\n",
    "expected_multinormal_pdf = np.array([8.70516304e-074, 8.62201632e-001, 5.36048920e+015, 2.99498046e+188,\n",
    "       6.91708798e+083, 9.96882978e-062, 7.03348279e-025, 2.16083146e-059,\n",
    "       1.87537738e-086, 1.84295981e+075, 1.11845126e+000, 5.17746613e-097])\n",
    "\n",
    "print(\"Your multinormal pdf works within the expected range: \", np.allclose(expected_multinormal_pdf, my_multinormalpdf))\n",
    "\n",
    "\n",
    "# test ll-joint\n",
    "sigma_now = sigma * 0.5\n",
    "my_lljoint = GMM(points, 3)._ll_joint(pi, mu, sigma_now, True)\n",
    "expected_lljoint = np.array([[ -8.48080757,  -7.51160532,  -2.81570712],\n",
    "       [ -7.28033641,  -2.81570712,  -7.51160531],\n",
    "       [ -5.99589887,  -4.74012597,  -8.87599209],\n",
    "       [ -7.22441107, -10.91289393,  -7.01960107],\n",
    "       [-11.20807212,  -7.65010086,  -5.98110204],\n",
    "       [ -6.92419072,  -4.83482203,  -3.56951111],\n",
    "       [ -7.72819284,  -3.06904217,  -6.26803946],\n",
    "       [ -8.41643028,  -3.87795155,  -4.485805  ],\n",
    "       [ -7.66746017,  -9.42125381,  -4.10677788],\n",
    "       [ -9.4646439 ,  -6.20890536,  -8.07178468],\n",
    "       [ -2.81570712,  -7.28033643,  -8.48080755],\n",
    "       [ -9.28269723, -12.03115847,  -4.17935163]])\n",
    "\n",
    "print(\"Your lljoint works within the expected range: \", np.allclose(my_lljoint, expected_lljoint))\n",
    "\n",
    "\n",
    "# test E step\n",
    "my_estep = GMM(points, 3)._E_step(pi, mu, sigma_now)\n",
    "expected_estep = np.array([[3.42169503e-03, 9.01904364e-03, 9.87559261e-01],\n",
    "       [1.12762023e-02, 9.79775837e-01, 8.94796041e-03],\n",
    "       [2.18977456e-01, 7.68731442e-01, 1.22911017e-02],\n",
    "       [4.43990237e-01, 1.11041589e-02, 5.44905604e-01],\n",
    "       [4.49802848e-03, 1.57844511e-01, 8.37657460e-01],\n",
    "       [2.65137773e-02, 2.14226353e-01, 7.59259870e-01],\n",
    "       [9.02095401e-03, 9.52129225e-01, 3.88498209e-02],\n",
    "       [6.87345697e-03, 6.43000762e-01, 3.50125781e-01],\n",
    "       [2.75025136e-02, 4.76112393e-03, 9.67736363e-01],\n",
    "       [3.22944111e-02, 8.37677122e-01, 1.30028467e-01],\n",
    "       [9.85247145e-01, 1.13391711e-02, 3.41368409e-03],\n",
    "       [6.03734929e-03, 3.86549176e-04, 9.93576102e-01]])\n",
    "\n",
    "\n",
    "print(\"Your E step works within the expected range: \", np.allclose(my_estep, expected_estep))\n",
    "\n",
    "# test M step\n",
    "my_pi, my_mu, my_sigma = GMM(points, 3)._M_step(expected_estep, True)\n",
    "expected_pi = np.array([0.1479711 , 0.38249961, 0.46952929])\n",
    "expected_mu = np.array([[-0.15519344,  1.22500376,  0.03548931],\n",
    "       [-0.36778399, -0.18068954, -0.65203503],\n",
    "       [-0.28448252, -0.09079301,  0.92618845]])\n",
    "expected_sigma = np.array([[[ 0.67247982, -0.25027742,  0.0774841 ],\n",
    "        [-0.25027742,  0.34077941,  0.04853111],\n",
    "        [ 0.0774841 ,  0.04853111,  0.2987035 ]],\n",
    "\n",
    "       [[ 0.49792869,  0.07842407, -0.09002534],\n",
    "        [ 0.07842407,  0.1618932 , -0.10696588],\n",
    "        [-0.09002534, -0.10696588,  0.20401203]],\n",
    "\n",
    "       [[ 0.65130447,  0.0049166 , -0.39258756],\n",
    "        [ 0.0049166 ,  0.22371688,  0.18769942],\n",
    "        [-0.39258756,  0.18769942,  0.70840301]]])\n",
    "print(\"Your M step works within the expected range: \", np.allclose(my_pi, expected_pi) and np.allclose(my_mu, expected_mu) and np.allclose(my_sigma, expected_sigma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Image Compression and pixel clustering [10pts + 5pts]\n",
    "\n",
    "Images typically need a lot of bandwidth to be transmitted over the network. In order to optimize this process, most image processors perform lossy compression of images (lossy implies some information is lost in the process of compression).\n",
    "\n",
    "In this section, you will use your GMM algorithm implementation to do pixel clustering and compress the images. That is to say, you would develop a lossy image compression algorithm. \n",
    "(Hint: you can adjust the number of clusters formed and justify your answer based on visual inspection of the resulting images or on a different metric of your choosing)\n",
    "\n",
    "\n",
    "**Special Notes**\n",
    "- Try to add a small value(e.g. SIGMA_CONST and LOG_CONST) before taking the operation if the output image is solid black.\n",
    "- The output images may be slightly different due to different initialization methods in GMM() function. \n",
    "    \n",
    "\n",
    "#### You do NOT need to submit your code for this question to the autograder. Instead you should include whatever images/information you find relevant in the report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "### NO NEED TO CHANGE THIS CELL ###\n",
    "###################################\n",
    "\n",
    "# helper function for performing pixel clustering. \n",
    "def cluster_pixels_gmm(image, K, full_matrix = True):\n",
    "    \"\"\"Clusters pixels in the input image\n",
    "    \n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "    Return:\n",
    "        clustered_img: image of shape(H, W, 3) after pixel clustering\n",
    "    \"\"\"\n",
    "    im_height, im_width, im_channel = image.shape\n",
    "    flat_img = np.reshape(image, [-1, im_channel]).astype(np.float32)\n",
    "    gamma, (pi, mu, sigma) = GMM(flat_img, K = K, max_iters = 10)(full_matrix)\n",
    "    cluster_ids = np.argmax(gamma, axis=1)\n",
    "    centers = mu\n",
    "\n",
    "    gmm_img = np.reshape(centers[cluster_ids], (im_height, im_width, im_channel))\n",
    "    \n",
    "    return gmm_img\n",
    "\n",
    "# helper function for plotting images. You don't have to modify it\n",
    "def plot_images(img_list, title_list, figsize=(20, 10)):\n",
    "    assert len(img_list) == len(title_list)\n",
    "    fig, axes = plt.subplots(1, len(title_list), figsize=figsize)\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(img_list[i] / 255.0)\n",
    "        ax.set_title(title_list[i])\n",
    "        ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the direction of two images. Both of them are from ImageNet\n",
    "img1_dir ='../data/images_for_GMM/bird.jpg'\n",
    "img2_dir ='../data/images_for_GMM/cat.jpg'\n",
    "\n",
    "# example of loading image \n",
    "image1 = imageio.imread('../data/images_for_GMM/bird.jpg')\n",
    "\n",
    "# this is for you to implement\n",
    "def perform_compression(image, min_clusters=5, max_clusters=15):\n",
    "    \"\"\"\n",
    "    Using the helper function above to find the optimal number of clusters that can appropriately produce a single image.\n",
    "    You can simply examinate the answer based on your visual inspection (i.e. looking at the resulting images) or provide any metrics you prefer. \n",
    "    \n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        min_clusters, max_clusters: the minimum and maximum number of clusters you should test with. Default are 5a dn 15.\n",
    "        (Usually the maximum number of clusters would not exeed 15)\n",
    "        \n",
    "    Return:\n",
    "        plot: comparison between original image and image pixel clustering.\n",
    "        optional: any other information/metric/plot you think is necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "image1 = imageio.imread(img1_dir)\n",
    "perform_compression(image1, 5, 10)\n",
    "\n",
    "image2 = imageio.imread(img2_dir)\n",
    "perform_compression(image2, 5, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Bonus for all) [5 pts]\n",
    "Compare full covariance matrix with diagonal covariance matrix. Can you explain why the images are different with same clusters?\n",
    "Note: You will have to implement both multinormalPDF and normalPDF, and add a few arguments in the original _ll_joint() and _Mstep() function. You will earn full credit only if you implement both functions AND explain the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "def compare_matrix(image, K):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: input image of shape(H, W, 3)\n",
    "        K: number of components\n",
    "        \n",
    "    Return:\n",
    "        plot: comparison between full covariance matrix and diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    #full covariance matrix\n",
    "    gmm_image_full = cluster_pixels_gmm(image, K, full_matrix = True)\n",
    "    #diagonal covariance matrix\n",
    "    gmm_image_diag = cluster_pixels_gmm(image, K, full_matrix = False)\n",
    "    \n",
    "    plot_images([gmm_image_full, gmm_image_diag], ['full covariance matrix', 'diagonal covariance matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "compare_matrix(image1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. (Bonus for All) Cleaning Messy data with semi-supervised learning [30pts]\n",
    "\n",
    "Learning to work with messy data is a hallmark of a well-rounded data scientist. In most real-world settings the data given will usually have some issue, so it is important to learn skills to work around such impasses. This part of the assignment looks to expose you to clever ways to fix data using concepts that you have already learned in the prior questions.\n",
    "\n",
    "#### Question\n",
    "\n",
    "After graduating from Georgia Tech with your shiny new degree, you are recruited to help with safety testing for the Mars rocket at NASA. Of course NASA won't be sending rocket after rocket to stress test your fellow employees' engineering (they also graduate from Tech, so you have full confidence in them), so instead, NASA has decided to run numerous simulations on the current engineering design of the Mars rocket. The simulation collects shuttle data from its sensors, resulting in 8 features which include bypass, rad flow, etc. These features are contained within the first through eighth columns. The ninth column shows the label with 1 being a successful simulation and 0 being an unsuccessful simulation. \n",
    "\n",
    "However, due to an intern accidentally deleting random data points, 20% of the entries are missing labels and 30% are missing characterization data. Since simply removing the corrupted entries would not reflect the true variance of the data, your job is to implement a solution to clean the data so it can be properly classified. \n",
    "\n",
    "Your job is to assist NASA in cleaning the data and implementing a semi-supervised learning framework to help them create a general classifier for future simulations.\n",
    "\n",
    "You are given two files for this task:\n",
    "* data.csv: the entire dataset with complete and incomplete data\n",
    "* validation.csv: a smaller, fully complete dataset made after after the intern deleted the datapoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.a Data Separating [3pt]\n",
    "The first step is to break up the whole dataset into clear parts. All the data is randomly shuffled in one csv file. In order to move forward, the data needs to be split into three separate arrays: \n",
    "* labeled_complete: containing the complete characterization data and corresponding labels \n",
    "* labeled_incomplete: containing partial characterization data (i.e., one of the features is NaN) and corresponding labels\n",
    "* unlabeled_complete: containing only complete material characterization results (i.e., the label is NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from semisupervised import complete_\n",
    "from semisupervised import incomplete_\n",
    "from semisupervised import unlabeled_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.b KNN [7pts]\n",
    "The second step in this task is to clean the Labeled_incomplete dataset by filling in the missing values with probable ones derived from complete data. A useful approach to this type of problem is using a k-nearest neighbors (k-NN) algorithm. For this application, the method consists of replacing the missing value of a given point with the mean of the closest k-neighbors to that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from semisupervised import CleanData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a good expectation of what the process should look like on a toy dataset. If your output matches the answer below, you are on the right track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "complete_data = np.array([[1.,2.,3.,1],[7.,8.,9.,0],[16.,17.,18.,1],[22.,23.,24.,0]])\n",
    "incomplete_data = np.array([[1.,np.nan,3.,1],[7.,np.nan,9.,0],[np.nan,17.,18.,1],[np.nan,23.,24.,0]])\n",
    "\n",
    "clean_data = CleanData()(incomplete_data, complete_data, 2)\n",
    "print(\"*** Expected Answer - k = 2 ***\")\n",
    "print(\"\"\"==complete data==\n",
    "[[ 1.  5.  3.  1.]\n",
    " [ 7.  8.  9.  0.]\n",
    " [16. 17. 18.  1.]\n",
    " [22. 23. 24.  0.]]\n",
    "==incomplete data==\n",
    "[[ 1. nan  3.  1.]\n",
    " [ 7. nan  9.  0.]\n",
    " [nan 17. 18.  1.]\n",
    " [nan 23. 24.  0.]]\n",
    "==clean_data==\n",
    "[[ 1.   2.   3.   1. ]\n",
    " [ 7.   8.   9.   0. ]\n",
    " [16.  17.  18.   1. ]\n",
    " [22.  23.  24.   0. ]\n",
    " [14.5 23.  24.   0. ]\n",
    " [ 7.  15.5  9.   0. ]\n",
    " [ 8.5 17.  18.   1. ]\n",
    " [ 1.   9.5  3.   1. ]]\"\"\")\n",
    "\n",
    "print(\"\\n*** My Answer - k = 2***\")\n",
    "print(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Getting acquainted with semi-supervised learning approaches. [5pts]\n",
    "\n",
    "You will implement a version of the algorithm presented in Table 1 of the paper [\"Text Classification from Labeled and Unlabeled Documents using EM\"](http://www.kamalnigam.com/papers/emcat-mlj99.pdf) by Nigam et al. (2000). While you are recommended to read the whole paper this assignment focuses on items 5.2 and 6.1. Write a brief summary of three interesting highlights of the paper (50-word maximum).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Implementing the EM algorithm. [10 pts]\n",
    "In your implementation of the EM algorithm proposed by Nigam et al. (2000) on Table 1, you will use a Gaussian Naive Bayes (GNB) classifier as opposed to a naive Bayes (NB) classifier. (Hint: Using a GNB in place of an NB will enable you to reuse most of the implementation you developed for GMM in this assignment. In fact, you can successfully solve the problem by simply modifying the call method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from semisupervised import SemiSupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Demonstrating the performance of the algorithm. [5pts]\n",
    "Compare the classification error based on the Gaussian Naive Bayes (GNB) classifier you implemented following the Nigam et al. (2000) approach to the performance of a GNB classifier trained using only labeled data. Since you have not covered supervised learning in class, you are allowed to use the scikit learn library for training the GNB classifier based only on labeled data: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html.\n",
    "\n",
    "To acheive the full 5 points you must get these scores:\n",
    "* semi_supervised_score > .87 \n",
    "* GNB_cleandata_score > .87 \n",
    "* GNB_onlycomplete_score > .87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from semisupervised import ComparePerformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load and clean data for the next section\n",
    "all_data = np.loadtxt('data.csv', delimiter=',')\n",
    "\n",
    "labeled_complete = complete_(all_data)\n",
    "labeled_incomplete = incomplete_(all_data)\n",
    "unlabeled = unlabeled_(all_data)\n",
    "\n",
    "clean_data = CleanData()(labeled_incomplete, labeled_complete, 10)\n",
    "# load unlabeled set\n",
    "# append unlabeled flag\n",
    "unlabeled = np.delete(unlabeled, -1, axis=1)\n",
    "unlabeled_flag = -1*np.ones((unlabeled.shape[0],1))\n",
    "unlabeled = np.concatenate((unlabeled, unlabeled_flag), 1)\n",
    "\n",
    "# =========================================================================\n",
    "# SEMI SUPERVISED\n",
    "\n",
    "# format training data\n",
    "points = np.concatenate((clean_data, unlabeled),0)\n",
    "# train model\n",
    "(pi, mu, sigma) = SemiSupervised()(points, 2)\n",
    "\n",
    "\n",
    "# =========================================================================\n",
    "# SUPERVISED WITH CLEAN DATA (SKLEARN)\n",
    "\n",
    "clean_clf = GaussianNB()\n",
    "clean_clf.fit(clean_data[:,:8], clean_data[:,8])\n",
    "\n",
    "# =========================================================================\n",
    "# SUPERVISED WITH ONLY THE COMPLETE DATA (SKLEARN)\n",
    "\n",
    "complete_clf = GaussianNB()\n",
    "complete_clf.fit(labeled_complete[:,:8], labeled_complete[:,8])\n",
    "\n",
    "# ==========================================================================\n",
    "# COMPARISON\n",
    "\n",
    "# load test data\n",
    "independent = np.loadtxt('validation.csv', delimiter=',')\n",
    "\n",
    "# classify test data\n",
    "classification = SemiSupervised()._E_step(independent[:,:8], pi, mu, sigma)\n",
    "classification = np.argmax(classification,axis=1)\n",
    "\n",
    "# =========================================================================================\n",
    "\n",
    "print(\"\"\"===COMPARISON===\"\"\")\n",
    "print(\"\"\"SemiSupervised Accuracy:\"\"\", ComparePerformance().accuracy_semi_supervised(classification, independent))\n",
    "print(\"\"\"Supervised with clean data: GNB Accuracy:\"\"\", ComparePerformance().accuracy_GNB_onlycomplete(labeled_complete, independent))\n",
    "print(\"\"\"Supervised with only complete data: GNB Accuracy:\"\"\", ComparePerformance().accuracy_GNB_cleandata(clean_data, independent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "371294a8f580372cda8a555fe0386cf2b270ba28c88268790cb700ff2771ecf8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
